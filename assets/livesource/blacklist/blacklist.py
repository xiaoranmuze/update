import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime
import os
from urllib.parse import urlparse
import socket
import subprocess
import traceback

timestart = datetime.now()
BlackHost = ["127.0.0.1:8080", "live3.lalifeier.eu.org", "newcntv.qcloudcdn.com"]

# æ–°å¢ï¼šå…¨å±€åˆ—è¡¨æ”¶é›†æ‰€æœ‰è¿è¡Œæ—¶ç»Ÿè®¡ä¿¡æ¯
runtime_stats = []

def read_txt_file(file_path):
    skip_strings = ['#genre#']
    required_strings = ['://']
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = [
            line for line in file
            if not any(skip_str in line for skip_str in skip_strings) and all(req_str in line for req_str in required_strings)
        ]
    return lines

def check_url(url, timeout=6):
    start_time = time.time()
    elapsed_time = None
    success = False
    encoded_url = urllib.parse.quote(url, safe=':/?&=')
    try:
        if url.startswith("http"):
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            }
            req = urllib.request.Request(encoded_url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                if response.status == 200:
                    success = True
        elif url.startswith("p3p"):
            success = check_p3p_url(url, timeout)
        elif url.startswith("p2p"):
            success = check_p2p_url(url, timeout)        
        elif url.startswith("rtmp") or url.startswith("rtsp"):
            success = check_rtmp_url(url, timeout)
        elif url.startswith("rtp"):
            success = check_rtp_url(url, timeout)
        elapsed_time = (time.time() - start_time) * 1000
    except Exception as e:
        print(f"Error checking {url}: {e}")
        record_host(get_host_from_url(url))
        elapsed_time = None
    return elapsed_time, success

def check_rtmp_url(url, timeout):
    try:
        result = subprocess.run(['ffprobe', url], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, Exception) as e:
        print(f"Error checking {url}: {e}")
    return False

def check_rtp_url(url, timeout):
    try:
        parsed_url = urlparse(url)
        host = parsed_url.hostname
        port = parsed_url.port
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.settimeout(timeout)
            s.connect((host, port))
            s.sendto(b'', (host, port))
            s.recv(1)
        return True
    except (socket.timeout, socket.error):
        return False

def check_p3p_url(url, timeout):
    try:
        parsed_url = urlparse(url)
        host = parsed_url.hostname
        port = parsed_url.port or (80 if parsed_url.scheme == "http" else 443)
        path = parsed_url.path or "/"
        if not host or not port or not path:
            raise ValueError("Invalid p3p URL")
        with socket.create_connection((host, port), timeout=timeout) as s:
            request = (
                f"GET {path} P3P/1.0\r\n"
                f"Host: {host}\r\n"
                f"User-Agent: CustomClient/1.0\r\n"
                f"Connection: close\r\n\r\n"
            )
            s.sendall(request.encode())
            response = s.recv(1024)
            return b"P3P" in response
    except Exception as e:
        print(f"Error checking {url}: {e}")
    return False

def check_p2p_url(url, timeout):
    try:
        parsed_url = urlparse(url)
        host = parsed_url.hostname
        port = parsed_url.port
        path = parsed_url.path
        if not host or not port or not path:
            raise ValueError("Invalid P2P URL")
        with socket.create_connection((host, port), timeout=timeout) as s:
            request = f"YOUR_CUSTOM_REQUEST {path}\r\nHost: {host}\r\n\r\n"
            s.sendall(request.encode())
            response = s.recv(1024)
            return b"SOME_EXPECTED_RESPONSE" in response
    except Exception as e:
        print(f"Error checking {url}: {e}")
    return False

def process_line(line):
    if "#genre#" in line or "://" not in line:
        return None, None
    parts = line.split(',')
    if len(parts) == 2:
        name, url = parts
        elapsed_time, is_valid = check_url(url.strip())
        if is_valid:
            return elapsed_time, line.strip()
        else:
            return None, line.strip()
    return None, None

def process_urls_multithreaded(lines, max_workers=30):
    blacklist = [] 
    successlist = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_line, line): line for line in lines}
        for future in as_completed(futures):
            elapsed_time, result = future.result()
            if result:
                if elapsed_time is not None:
                    successlist.append(f"{elapsed_time:.2f}ms,{result}")
                else:
                    blacklist.append(result)
    return successlist, blacklist

def write_list(file_path, data_list):
    with open(file_path, 'w', encoding='utf-8') as file:
        for item in data_list:
            file.write(item + '\n')

def get_url_file_extension(url):
    parsed_url = urlparse(url)
    path = parsed_url.path
    return os.path.splitext(path)[1]

def convert_m3u_to_txt(m3u_content):
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith("http"):
            txt_lines.append(f"{channel_name},{line.strip()}")
    return txt_lines

def process_url(url):
    try:
        with urllib.request.urlopen(url) as response:
            data = response.read()
            text = data.decode('utf-8')
            if get_url_file_extension(url) in [".m3u", ".m3u8"]:
                m3u_lines = convert_m3u_to_txt(text)
                stats = f"{len(m3u_lines)},{url.strip()}"
                url_statistics.append(stats)
                runtime_stats.append(f"è¿œç¨‹è®¢é˜…ç»Ÿè®¡: {stats}")  # æ”¶é›†ç»Ÿè®¡
                urls_all_lines.extend(m3u_lines)
            elif get_url_file_extension(url) == ".txt":
                lines = text.split('\n')
                valid_lines = [line.strip() for line in lines if "#genre#" not in line and "," in line and "://" in line]
                stats = f"{len(valid_lines)},{url.strip()}"
                url_statistics.append(stats)
                runtime_stats.append(f"è¿œç¨‹è®¢é˜…ç»Ÿè®¡: {stats}")  # æ”¶é›†ç»Ÿè®¡
                urls_all_lines.extend(valid_lines)
    except Exception as e:
        err_msg = f"å¤„ç†URL[{url}]å¤±è´¥: {e}"
        print(err_msg)
        runtime_stats.append(err_msg)  # æ”¶é›†é”™è¯¯ç»Ÿè®¡

def remove_duplicates_url(lines):
    urls = []
    newlines = []
    for line in lines:
        if "," in line and "://" in line:
            channel_url = line.split(',')[1].strip()
            if channel_url not in urls:
                urls.append(channel_url)
                newlines.append(line)
    return newlines

def clean_url(lines):
    newlines = []
    for line in lines:
        if "," in line and "://" in line:
            last_dollar_index = line.rfind('$')
            if last_dollar_index != -1:
                line = line[:last_dollar_index]
            newlines.append(line)
    return newlines

def split_url(lines):
    newlines = []
    for line in lines:
        if "," in line:
            channel_name, channel_address = line.split(',', 1)
            if "#" not in channel_address:
                newlines.append(line)
            elif "#" in channel_address and "://" in channel_address:
                url_list = channel_address.split('#')
                for url in url_list:
                    if "://" in url:
                        newlines.append(line)
    return newlines

def get_host_from_url(url: str) -> str:
    try:
        parsed_url = urlparse(url)
        return parsed_url.netloc
    except Exception as e:
        return f"Error: {str(e)}"

blacklist_dict = {}
def record_host(host):
    if host in blacklist_dict:
        blacklist_dict[host] += 1
    else:
        blacklist_dict[host] = 1

if __name__ == "__main__":
    urls_all_lines = []
    url_statistics = []
    runtime_stats.append(f"===== æ£€æµ‹å¼€å§‹: {timestart.strftime('%Y%m%d %H:%M:%S')} =====")  # åˆå§‹ç»Ÿè®¡

    try:
        # è¿œç¨‹è®¢é˜…URLå¤„ç†
        urls = [
            "https://raw.githubusercontent.com/xiaoran67/update/refs/heads/main/output/full.txt",
            "https://raw.githubusercontent.com/xiaoran67/update/refs/heads/main/output/result.txt"
        ]
        for url in urls:
            msg = f"å¤„ç†è¿œç¨‹URL: {url}"
            print(msg)
            runtime_stats.append(msg)  # æ”¶é›†è¿œç¨‹å¤„ç†ç»Ÿè®¡
            process_url(url)

        # è·å–å½“å‰è„šæœ¬ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))  
        runtime_stats.append(f"è„šæœ¬ç›®å½•: {current_dir}")  # æ”¶é›†ç›®å½•ç»Ÿè®¡

        # æœ¬åœ°æ–‡ä»¶å¤„ç†
        input_black_file = os.path.join(current_dir, 'blacklist_auto.txt')
        lines_black = read_txt_file(input_black_file) if os.path.exists(input_black_file) else []
        msg = f"æœ¬åœ°blackæ–‡ä»¶è¡Œæ•°: {len(lines_black)}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†æœ¬åœ°æ–‡ä»¶ç»Ÿè®¡

        # åˆå¹¶è¾“å…¥æº
        lines = urls_all_lines + lines_black
        urls_hj_before = len(lines)
        msg = f"åˆå§‹æ€»è¡Œæ•°ï¼ˆè¿œç¨‹+æœ¬åœ°ï¼‰: {urls_hj_before}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†åˆå§‹è¡Œæ•°ç»Ÿè®¡

        # æ•°æ®æ¸…æ´—æµç¨‹
        lines = split_url(lines)
        urls_hj_before2 = len(lines)
        msg = f"åˆ†è§£#åè¡Œæ•°: {urls_hj_before2}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†åˆ†è§£åç»Ÿè®¡

        lines = clean_url(lines)
        urls_hj_before3 = len(lines)
        msg = f"å»$åè¡Œæ•°: {urls_hj_before3}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†å»$åç»Ÿè®¡

        lines = remove_duplicates_url(lines)
        urls_hj = len(lines)
        msg = f"å»é‡åè¡Œæ•°: {urls_hj}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†å»é‡åç»Ÿè®¡

        # å¤šçº¿ç¨‹æ£€æµ‹
        CONCURRENT_WORKERS = 30
        msg = f"å¹¶å‘çº¿ç¨‹æ•°: {CONCURRENT_WORKERS}"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†çº¿ç¨‹æ•°ç»Ÿè®¡

        successlist, blacklist = process_urls_multithreaded(lines, max_workers=CONCURRENT_WORKERS)
        urls_ok = len(successlist)
        urls_ng = len(blacklist)
        msg = f"æ£€æµ‹ç»“æœ: æˆåŠŸ{urls_ok}æ¡, å¤±è´¥{urls_ng}æ¡"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†æ£€æµ‹ç»“æœç»Ÿè®¡

        # ç»“æœæ•´ç†ä¸ä¿å­˜
        def remove_prefix_from_lines(lines):
            result = []
            for line in lines:
                if "#genre#" not in line and "," in line and "://" in line:
                    parts = line.split(",")
                    result.append(",".join(parts[1:]))
            return result

        version = datetime.now().strftime("%Y%m%d-%H-%M-%S") + ",url"
        successlist_tv = ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] + ["whitelist,#genre#"] + remove_prefix_from_lines(successlist)
        successlist = ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] + ["RespoTime,whitelist,#genre#"] + successlist
        blacklist = ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] + ["blacklist,#genre#"] + blacklist

        success_file = os.path.join(current_dir, 'whitelist_auto.txt')
        success_file_tv = os.path.join(current_dir, 'whitelist_auto_tv.txt')
        blacklist_file = os.path.join(current_dir, 'blacklist_auto.txt')
        write_list(success_file, successlist)
        write_list(success_file_tv, successlist_tv)
        write_list(blacklist_file, blacklist)
        runtime_stats.append(f"ç»“æœæ–‡ä»¶ä¿å­˜: {success_file}, {success_file_tv}, {blacklist_file}")  # æ”¶é›†æ–‡ä»¶ä¿å­˜ç»Ÿè®¡

        # å†å²è®°å½•ä¿å­˜
        timenow = datetime.now().strftime("%Y%m%d_%H_%M_%S")
        history_dir = os.path.join(current_dir, "history", "blacklist")
        os.makedirs(history_dir, exist_ok=True)
        history_success_file = os.path.join(history_dir, f"{timenow}_whitelist_auto.txt")
        history_blacklist_file = os.path.join(history_dir, f"{timenow}_blacklist_auto.txt")
        write_list(history_success_file, successlist)
        write_list(history_blacklist_file, blacklist)
        runtime_stats.append(f"å†å²è®°å½•ä¿å­˜: {history_success_file}, {history_blacklist_file}")  # æ”¶é›†å†å²è®°å½•ç»Ÿè®¡

        # æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
        timeend = datetime.now()
        elapsed_time = timeend - timestart
        total_seconds = elapsed_time.total_seconds()
        minutes = int(total_seconds // 60)
        seconds = int(total_seconds % 60)
        timestart_str = timestart.strftime("%Y%m%d_%H_%M_%S")
        timeend_str = timeend.strftime("%Y%m%d_%H_%M_%S")
        msg = f"æ‰§è¡Œæ—¶é—´: {minutes}åˆ†{seconds}ç§’ (å¼€å§‹: {timestart_str}, ç»“æŸ: {timeend_str})"
        print(msg)
        runtime_stats.append(msg)  # æ”¶é›†æ—¶é—´ç»Ÿè®¡

        # æ•…éšœä¸»æœºç»Ÿè®¡
        blackhost_dir = os.path.join(current_dir, "blackhost")
        os.makedirs(blackhost_dir, exist_ok=True)
        blackhost_filename = os.path.join(blackhost_dir, f"{timenow}_blackhost_count.txt")
        def save_blackhost_to_txt(filename=blackhost_filename):
            with open(filename, "w") as file:
                for host, count in blacklist_dict.items():
                    file.write(f"{host}: {count}\n")
            return filename
        saved_host_file = save_blackhost_to_txt()
        runtime_stats.append(f"æ•…éšœä¸»æœºç»Ÿè®¡ä¿å­˜: {saved_host_file}")  # æ”¶é›†æ•…éšœä¸»æœºç»Ÿè®¡

        # ğŸŒŸ å†™å…¥æ‰€æœ‰ç»Ÿè®¡åˆ°æ—¥å¿—æ–‡ä»¶
        stats_file = os.path.join(current_dir, 'url_statistics.log')
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"# å®Œæ•´æ£€æµ‹ç»Ÿè®¡æ—¥å¿—\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y%m%d %H:%M:%S')}\n")
            f.write("\n".join(runtime_stats))  # å†™å…¥æ‰€æœ‰æ”¶é›†çš„ç»Ÿè®¡
        msg = f"âœ… æ‰€æœ‰ç»Ÿè®¡å·²å†™å…¥æ—¥å¿—: {stats_file}"
        print(msg)

    except Exception as e:
        error_time = datetime.now().strftime("%Y%m%d %H:%M:%S")
        err_msg = f"âŒ ä¸»ç¨‹åºå¼‚å¸¸: {str(e)}"
        print(err_msg)
        runtime_stats.append(f"\n# æ£€æµ‹å¼‚å¸¸ (æ—¶é—´: {error_time})")
        runtime_stats.append(f"# é”™è¯¯åŸå› : {str(e)}")
        runtime_stats.append(f"# å †æ ˆä¿¡æ¯: {traceback.format_exc()}")

        # å¼‚å¸¸æ—¶å†™å…¥ç»Ÿè®¡æ—¥å¿—
        stats_file = os.path.join(current_dir, 'url_statistics.log')
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"# å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡æ—¥å¿—\n")
            f.write(f"# å¼‚å¸¸æ—¶é—´: {error_time}\n")
            f.write("\n".join(runtime_stats))  # å†™å…¥å¼‚å¸¸å‰æ”¶é›†çš„ç»Ÿè®¡+é”™è¯¯ä¿¡æ¯
        msg = f"âš ï¸ å¼‚å¸¸ç»Ÿè®¡å·²å†™å…¥æ—¥å¿—: {stats_file}"
        print(msg)

        # è¦†ç›–æ ¸å¿ƒç»“æœæ–‡ä»¶ä¸ºé”™è¯¯ä¿¡æ¯
        error_lines = [f"CheckTimeï¼š{error_time}", f"ERRORï¼š{str(e)}"]
        for path in [
            os.path.join(current_dir, 'whitelist_auto.txt'),
            os.path.join(current_dir, 'whitelist_auto_tv.txt'),
            os.path.join(current_dir, 'blacklist_auto.txt')
        ]:
            write_list(path, error_lines)
