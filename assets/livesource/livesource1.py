#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ====== ç›´æ’­æºèšåˆå¤„ç†å·¥å…· v1.00 ======
# ======= LiveSource-Collector =======
# ===========  åŸºç¡€ç‰ˆ============
import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc

import socket
import time

# åˆå§‹åŒ–è¾“å‡ºç›®å½•
os.makedirs('output', exist_ok=True)  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸ä¼šæŠ¥é”™
print(f"åˆ›å»ºè¾“å‡ºç›®å½•: output")

# ç¹ä½“è½¬ç®€ä½“å‡½æ•°
def traditional_to_simplified(text: str) -> str:
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text

# æ‰“å°ç‰ˆæœ¬è¯´æ˜
print()
print(f"=" * 31)
print(f"ğŸ IPTVç›´æ’­æºèšåˆå¤„ç†å·¥å…· v1.00")
print(f"ğŸ“º Live Source Collector Processing")
print(f"ğŸ”§ åŸºäºv0.00ï¼ŒåŸºç¡€ç‰ˆ")
print(f"=" * 31)

# è®°å½•è„šæœ¬å¼€å§‹æ‰§è¡Œçš„æ—¶é—´
timestart = datetime.now()
print(f"\nâ° å¼€å§‹æ—¶é—´: {timestart.strftime('%Y%m%d %H:%M:%S')}")

# è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹åˆ°æ•°ç»„çš„å‡½æ•°
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#')]
            return lines

    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯ {file_name}: {e}")
        return []

# ä»æ–‡æœ¬æ–‡ä»¶è¯»å–é»‘åå•çš„å‡½æ•°
def read_blacklist_from_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    BlackList = [line.split(',')[1].strip() for line in lines if ',' in line]
    return BlackList

# è¯»å–è‡ªåŠ¨å’Œæ‰‹åŠ¨ç»´æŠ¤çš„é»‘åå•ï¼Œåˆå¹¶ä¸ºé›†åˆå»é‡
blacklist_auto=read_blacklist_from_txt('assets/livesource/blacklist/blacklist_auto.txt') 
blacklist_manual=read_blacklist_from_txt('assets/livesource/blacklist/blacklist_manual.txt') 
combined_blacklist = set(blacklist_auto + blacklist_manual)

print(f"\nğŸ”´ é»‘åå•ç»Ÿè®¡ä¿¡æ¯:")
print(f"   ğŸ”§ è‡ªåŠ¨ç»´æŠ¤: {len(blacklist_auto)} æ¡")
print(f"   âœï¸ æ‰‹åŠ¨ç»´æŠ¤: {len(blacklist_manual)} æ¡")
print(f"   ğŸ”„ åˆå¹¶å»é‡: {len(combined_blacklist)} æ¡")

# æ˜¾ç¤ºå‰å‡ æ¡é»‘åå•ç¤ºä¾‹
print(f"   é»‘åå•ç¤ºä¾‹ (å‰3æ¡):")
for i, url in enumerate(list(combined_blacklist)[:3]):
    print(f"     {i+1}. {url}")
if len(combined_blacklist) > 3:
    print(f"     ... è¿˜æœ‰ {len(combined_blacklist) - 3} æ¡")

# ========= é¢‘é“åˆ†ç±»å­˜å‚¨å˜é‡å®šä¹‰ =========
# åˆå§‹åŒ–å„ç§é¢‘é“ç±»åˆ«çš„ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¯¹åº”é¢‘é“çš„æ’­æ”¾æºä¿¡æ¯

yangshi_lines = []      # å­˜å‚¨å¤®è§†é¢‘é“æ•°æ®
weishi_lines = []       # å­˜å‚¨å«è§†é¢‘é“æ•°æ®

beijing_lines = []      # åŒ—äº¬
shanghai_lines = []     # ä¸Šæµ·
guangdong_lines = []    # å¹¿ä¸œ
jiangsu_lines = []      # æ±Ÿè‹
zhejiang_lines = []     # æµ™æ±Ÿ
shandong_lines = []     # å±±ä¸œ
sichuan_lines = []      # å››å·
henan_lines = []        # æ²³å—
hunan_lines = []        # æ¹–å—
chongqing_lines = []    # é‡åº†
tianjin_lines = []      # å¤©æ´¥
hubei_lines = []        # æ¹–åŒ—
anhui_lines = []        # å®‰å¾½
fujian_lines = []       # ç¦å»º
liaoning_lines = []     # è¾½å®
shaanxi_lines = []      # é™•è¥¿
hebei_lines = []        # æ²³åŒ—
jiangxi_lines = []      # æ±Ÿè¥¿
guangxi_lines = []      # å¹¿è¥¿
yunnan_lines = []       # äº‘å—
shanxi_lines = []       # å±±è¥¿
heilongjiang_lines = [] # é»‘é¾™æ±Ÿ
jilin_lines = []        # å‰æ—
guizhou_lines = []      # è´µå·
gansu_lines = []        # ç”˜è‚ƒ
neimenggu_lines = []    # å†…è’™å¤
xinjiang_lines = []     # æ–°ç–†
hainan_lines = []       # æµ·å—
ningxia_lines = []      # å®å¤
qinghai_lines = []      # é’æµ·
xizang_lines = []       # è¥¿è—

hongkong_lines = []   # é¦™æ¸¯
macau_lines = []      # æ¾³é—¨
taiwan_lines = []      # å°æ¹¾

digital_lines = []     # æ•°å­—
movie_lines = []       # ç”µå½±
tv_drama_lines = []    # ç”µè§†å‰§
documentary_lines = [] # çºªå½•ç‰‡
cartoon_lines = []     # åŠ¨ç”»ç‰‡
radio_lines = []       # æ”¶éŸ³æœº
variety_lines = []     # ç»¼è‰º
huya_lines = []        # è™ç‰™
douyu_lines = []       # æ–—é±¼
commentary_lines = []  # è§£è¯´
music_lines = []       # éŸ³ä¹
food_lines = []        # ç¾é£Ÿ
travel_lines = []      # æ—…æ¸¸
health_lines = []      # å¥åº·
finance_lines = []     # è´¢ç»
shopping_lines = []    # è´­ç‰©
game_lines = []        # æ¸¸æˆ
news_lines = []        # æ–°é—»
china_lines = []       # ä¸­å›½
international_lines = [] # å›½é™…
sports_lines = []      # ä½“è‚²
tyss_lines = []        # ä½“è‚²èµ›äº‹
mgss_lines = []        # å’ªå’•èµ›äº‹
traditional_opera_lines = [] # æˆæ›²é¢‘é“
spring_festival_gala_lines = [] # å†å±Šæ˜¥æ™š
camera_lines = []      # æ™¯åŒºç›´æ’­
favorite_lines = []    # æ”¶è—é¢‘é“

other_lines = []       # å…¶ä»–æœªåˆ†ç±»é¢‘é“
other_lines_url = []    # å…¶ä»–é¢‘é“çš„URLåˆ—è¡¨

# å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²çš„å‡½æ•°ï¼ˆä¸»è¦ç”¨äºå¤„ç†CCTVé¢‘é“åï¼‰
def process_name_string(input_str):
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

# å¤„ç†å•ä¸ªé¢‘é“åç§°éƒ¨åˆ†çš„å‡½æ•°
def process_part(part_str):
    if "CCTV" in part_str  and "://" not in part_str:
        part_str=part_str.replace("IPV6", "")
        part_str=part_str.replace("PLUS", "+")
        part_str=part_str.replace("1080", "")
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        if not filtered_str.strip():
            filtered_str=part_str.replace("CCTV", "")
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2: 
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
        return "CCTV"+filtered_str
    elif "å«è§†" in part_str:
        pattern = r'å«è§†ã€Œ.*ã€'
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    return part_str

# è·å–URLæ–‡ä»¶æ‰©å±•åçš„å‡½æ•°
def get_url_file_extension(url):
    parsed_url = urlparse(url)
    path = parsed_url.path
    extension = os.path.splitext(path)[1]
    return extension

# å°†M3Uæ ¼å¼è½¬æ¢ä¸ºTXTæ ¼å¼çš„å‡½æ•°
def convert_m3u_to_txt(m3u_content):
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p") :
            txt_lines.append(f"{channel_name},{line.strip()}")
        if "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    return '\n'.join(txt_lines)

# æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨çš„å‡½æ•°
def check_url_existence(data_list, url):
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls

# æ¸…ç†URLçš„å‡½æ•°ï¼ˆç§»é™¤$åçš„å‚æ•°ï¼‰
def clean_url(url):
    last_dollar_index = url.rfind('$')
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# éœ€è¦ä»é¢‘é“åç§°ä¸­ç§»é™¤çš„å­—ç¬¦ä¸²åˆ—è¡¨
removal_list = ["_ç”µä¿¡","ç”µä¿¡","é¢‘é“","é¢‘é™†","å¤‡é™†","å£¹é™†","è´°é™†","åé™†","è‚†é™†","ä¼é™†","é™†é™†","æŸ’é™†",
    "è‚†æŸ’","é¢‘è‹±","é¢‘ç‰¹","é¢‘å›½","é¢‘æ™´","é¢‘ç²¤","é«˜æ¸…","è¶…æ¸…","æ ‡æ¸…","æ–¯ç‰¹","ç²¤é™†","å›½é™†","é¢‘å£¹","é¢‘è´°",
    "è‚†è´°","é¢‘æµ‹","å’ªå’•","é—½ç‰¹","é«˜ç‰¹","é¢‘é«˜","é¢‘æ ‡","æ±é˜³","é¢‘æ•ˆ","å›½æ ‡","ç²¤æ ‡","é¢‘æ¨","é¢‘æµ","ç²¤é«˜",
    "é¢‘é™","å®æ—¶","ç¾æ¨","é¢‘ç¾","è‹±é™†","(åŒ—ç¾)","ã€Œå›çœ‹ã€","[è¶…æ¸…]","ã€ŒIPV4ã€","ã€ŒIPV6ã€","_ITV","(HK)",
    "AKtv","HD","[HD]","(HD)","ï¼ˆHDï¼‰","{HD}","<HD>","-HD","[BD]","SD","[SD]","(SD)","{SD}", "<SD>",
    "[VGA]","4Gtv","1080","720","480","VGA","4K","(4K)","{4K}","<4K>","(VGA)","{VGA}","<VGA>",
    "ã€Œ4gTVã€","ã€ŒLiTVã€"]

# æ¸…ç†é¢‘é“åç§°çš„å‡½æ•°
def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]
    return channel_name

# å¤„ç†å•è¡Œé¢‘é“ä¿¡æ¯çš„å‡½æ•°
def process_channel_line(line):
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0].strip()
        channel_name = clean_channel_name(channel_name, removal_list)
        channel_name = traditional_to_simplified(channel_name)
        channel_address = clean_url(line.split(',')[1].strip())
        line = channel_name + "," + channel_address
        if channel_address not in combined_blacklist:
            # å¤®è§†
            if "CCTV" in channel_name and check_url_existence(yangshi_lines, channel_address):
                yangshi_lines.append(process_name_string(line.strip()))
            # å«è§†
            elif channel_name in weishi_dictionary and check_url_existence(weishi_lines, channel_address):
                weishi_lines.append(process_name_string(line.strip()))
            # åŒ—äº¬
            elif channel_name in beijing_dictionary and check_url_existence(beijing_lines, channel_address):
                beijing_lines.append(process_name_string(line.strip()))
            # ä¸Šæµ·
            elif channel_name in shanghai_dictionary and check_url_existence(shanghai_lines, channel_address):
                shanghai_lines.append(process_name_string(line.strip()))
            # å¹¿ä¸œ
            elif channel_name in guangdong_dictionary and check_url_existence(guangdong_lines, channel_address):
                guangdong_lines.append(process_name_string(line.strip()))
            # æ±Ÿè‹
            elif channel_name in jiangsu_dictionary and check_url_existence(jiangsu_lines, channel_address):
                jiangsu_lines.append(process_name_string(line.strip()))
            # æµ™æ±Ÿ
            elif channel_name in zhejiang_dictionary and check_url_existence(zhejiang_lines, channel_address):
                zhejiang_lines.append(process_name_string(line.strip()))
            # å±±ä¸œ
            elif channel_name in shandong_dictionary and check_url_existence(shandong_lines, channel_address):
                shandong_lines.append(process_name_string(line.strip()))
            # å››å·
            elif channel_name in sichuan_dictionary and check_url_existence(sichuan_lines, channel_address):
                sichuan_lines.append(process_name_string(line.strip()))
            # æ²³å—
            elif channel_name in henan_dictionary and check_url_existence(henan_lines, channel_address):
                henan_lines.append(process_name_string(line.strip()))
            # æ¹–å—
            elif channel_name in hunan_dictionary and check_url_existence(hunan_lines, channel_address):
                hunan_lines.append(process_name_string(line.strip()))
            # é‡åº†
            elif channel_name in chongqing_dictionary and check_url_existence(chongqing_lines, channel_address):
                chongqing_lines.append(process_name_string(line.strip()))
            # å¤©æ´¥
            elif channel_name in tianjin_dictionary and check_url_existence(tianjin_lines, channel_address):
                tianjin_lines.append(process_name_string(line.strip()))
            # æ¹–åŒ—
            elif channel_name in hubei_dictionary and check_url_existence(hubei_lines, channel_address):
                hubei_lines.append(process_name_string(line.strip()))
            # å®‰å¾½
            elif channel_name in anhui_dictionary and check_url_existence(anhui_lines, channel_address):
                anhui_lines.append(process_name_string(line.strip()))
            # ç¦å»º
            elif channel_name in fujian_dictionary and check_url_existence(fujian_lines, channel_address):
                fujian_lines.append(process_name_string(line.strip()))
            # è¾½å®
            elif channel_name in liaoning_dictionary and check_url_existence(liaoning_lines, channel_address):
                liaoning_lines.append(process_name_string(line.strip()))
            # é™•è¥¿
            elif channel_name in shaanxi_dictionary and check_url_existence(shaanxi_lines, channel_address):
                shaanxi_lines.append(process_name_string(line.strip()))
            # æ²³åŒ—
            elif channel_name in hebei_dictionary and check_url_existence(hebei_lines, channel_address):
                hebei_lines.append(process_name_string(line.strip()))
            # æ±Ÿè¥¿
            elif channel_name in jiangxi_dictionary and check_url_existence(jiangxi_lines, channel_address):
                jiangxi_lines.append(process_name_string(line.strip()))
            # å¹¿è¥¿
            elif channel_name in guangxi_dictionary and check_url_existence(guangxi_lines, channel_address):
                guangxi_lines.append(process_name_string(line.strip()))
            # äº‘å—
            elif channel_name in yunnan_dictionary and check_url_existence(yunnan_lines, channel_address):
                yunnan_lines.append(process_name_string(line.strip()))
            # å±±è¥¿
            elif channel_name in shanxi_dictionary and check_url_existence(shanxi_lines, channel_address):
                shanxi_lines.append(process_name_string(line.strip()))
            # é»‘é¾™æ±Ÿ
            elif channel_name in heilongjiang_dictionary and check_url_existence(heilongjiang_lines, channel_address):
                heilongjiang_lines.append(process_name_string(line.strip()))
            # å‰æ—
            elif channel_name in jilin_dictionary and check_url_existence(jilin_lines, channel_address):
                jilin_lines.append(process_name_string(line.strip()))
            # è´µå·
            elif channel_name in guizhou_dictionary and check_url_existence(guizhou_lines, channel_address):
                guizhou_lines.append(process_name_string(line.strip()))
            # ç”˜è‚ƒ
            elif channel_name in gansu_dictionary and check_url_existence(gansu_lines, channel_address):
                gansu_lines.append(process_name_string(line.strip()))
            # å†…è’™å¤
            elif channel_name in neimenggu_dictionary and check_url_existence(neimenggu_lines, channel_address):
                neimenggu_lines.append(process_name_string(line.strip()))
            # æ–°ç–†
            elif channel_name in xinjiang_dictionary and check_url_existence(xinjiang_lines, channel_address):
                xinjiang_lines.append(process_name_string(line.strip()))
            # æµ·å—
            elif channel_name in hainan_dictionary and check_url_existence(hainan_lines, channel_address):
                hainan_lines.append(process_name_string(line.strip()))
            # å®å¤
            elif channel_name in ningxia_dictionary and check_url_existence(ningxia_lines, channel_address):
                ningxia_lines.append(process_name_string(line.strip()))
            # é’æµ·
            elif channel_name in qinghai_dictionary and check_url_existence(qinghai_lines, channel_address):
                qinghai_lines.append(process_name_string(line.strip()))
            # è¥¿è—
            elif channel_name in xizang_dictionary and check_url_existence(xizang_lines, channel_address):
                xizang_lines.append(process_name_string(line.strip()))
            # é¦™æ¸¯
            elif channel_name in hongkong_dictionary and check_url_existence(hongkong_lines, channel_address):
                hongkong_lines.append(process_name_string(line.strip()))
            # æ¾³é—¨
            elif channel_name in macau_dictionary and check_url_existence(macau_lines, channel_address):
                macau_lines.append(process_name_string(line.strip()))
            # å°æ¹¾
            elif channel_name in taiwan_dictionary and check_url_existence(taiwan_lines, channel_address):
                taiwan_lines.append(process_name_string(line.strip()))
            # æ•°å­—
            elif channel_name in digital_dictionary and check_url_existence(digital_lines, channel_address):
                digital_lines.append(process_name_string(line.strip()))
            # ç”µå½±
            elif channel_name in movie_dictionary and check_url_existence(movie_lines, channel_address):
                movie_lines.append(process_name_string(line.strip()))
            # ç”µè§†å‰§
            elif channel_name in tv_drama_dictionary and check_url_existence(tv_drama_lines, channel_address):
                tv_drama_lines.append(process_name_string(line.strip()))
            # çºªå½•ç‰‡
            elif channel_name in documentary_dictionary and check_url_existence(documentary_lines, channel_address):
                documentary_lines.append(process_name_string(line.strip()))
            # åŠ¨ç”»ç‰‡
            elif channel_name in cartoon_dictionary and check_url_existence(cartoon_lines, channel_address):
                cartoon_lines.append(process_name_string(line.strip()))
            # æ”¶éŸ³æœº
            elif channel_name in radio_dictionary and check_url_existence(radio_lines, channel_address):
                radio_lines.append(process_name_string(line.strip()))
            # ç»¼è‰º
            elif channel_name in variety_dictionary and check_url_existence(variety_lines, channel_address):
                variety_lines.append(process_name_string(line.strip()))
            # è™ç‰™
            elif channel_name in huya_dictionary and check_url_existence(huya_lines, channel_address):
                huya_lines.append(process_name_string(line.strip()))
            # æ–—é±¼
            elif channel_name in douyu_dictionary and check_url_existence(douyu_lines, channel_address):
                douyu_lines.append(process_name_string(line.strip()))
            # è§£è¯´
            elif channel_name in commentary_dictionary and check_url_existence(commentary_lines, channel_address):
                commentary_lines.append(process_name_string(line.strip()))
            # éŸ³ä¹
            elif channel_name in music_dictionary and check_url_existence(music_lines, channel_address):
                music_lines.append(process_name_string(line.strip()))
            # ç¾é£Ÿ
            elif channel_name in food_dictionary and check_url_existence(food_lines, channel_address):
                food_lines.append(process_name_string(line.strip()))
            # æ—…æ¸¸
            elif channel_name in travel_dictionary and check_url_existence(travel_lines, channel_address):
                travel_lines.append(process_name_string(line.strip()))
            # å¥åº·
            elif channel_name in health_dictionary and check_url_existence(health_lines, channel_address):
                health_lines.append(process_name_string(line.strip()))
            # è´¢ç»
            elif channel_name in finance_dictionary and check_url_existence(finance_lines, channel_address):
                finance_lines.append(process_name_string(line.strip()))
            # è´­ç‰©
            elif channel_name in shopping_dictionary and check_url_existence(shopping_lines, channel_address):
                shopping_lines.append(process_name_string(line.strip()))
            # æ¸¸æˆ
            elif channel_name in game_dictionary and check_url_existence(game_lines, channel_address):
                game_lines.append(process_name_string(line.strip()))
            # æ–°é—»
            elif channel_name in news_dictionary and check_url_existence(news_lines, channel_address):
                news_lines.append(process_name_string(line.strip()))
            # ä¸­å›½
            elif channel_name in china_dictionary and check_url_existence(china_lines, channel_address):
                china_lines.append(process_name_string(line.strip()))
            # å›½é™…
            elif channel_name in international_dictionary and check_url_existence(international_lines, channel_address):
                international_lines.append(process_name_string(line.strip()))
            # ä½“è‚²
            elif channel_name in sports_dictionary and check_url_existence(sports_lines, channel_address):
                sports_lines.append(process_name_string(line.strip()))
            # ä½“è‚²èµ›äº‹
#            elif any(tyss_dictionary in channel_name for tyss_dictionary in tyss_dictionary) and check_url_existence(tyss_lines, channel_address):  #ä½“è‚²èµ›äº‹ï¼ˆ2025æ–°å¢ï¼‰
#                tyss_lines.append(process_name_string(line.strip()))
            elif any(tyss_keyword in channel_name for tyss_keyword in tyss_dictionary) and check_url_existence(tyss_lines, channel_address):
                tyss_lines.append(process_name_string(line.strip()))
            # å’ªå’•èµ›äº‹
#            elif any(mgss_dictionary in channel_name for mgss_dictionary in mgss_dictionary) and check_url_existence(mgss_lines, channel_address):  #å’ªå’•èµ›äº‹ï¼ˆ2025æ–°å¢ï¼‰
#                mgss_lines.append(process_name_string(line.strip()))
            elif any(mgss_keyword in channel_name for mgss_keyword in mgss_dictionary) and check_url_existence(mgss_lines, channel_address):
                mgss_lines.append(process_name_string(line.strip()))
            # æˆæ›²é¢‘é“
            elif channel_name in traditional_opera_dictionary and check_url_existence(traditional_opera_lines, channel_address):
                traditional_opera_lines.append(process_name_string(line.strip()))
            # å†å±Šæ˜¥æ™š
            elif channel_name in spring_festival_gala_dictionary and check_url_existence(spring_festival_gala_lines, channel_address):
                spring_festival_gala_lines.append(process_name_string(line.strip()))
            # æ™¯åŒºç›´æ’­
            elif channel_name in camera_dictionary and check_url_existence(camera_lines, channel_address):
                camera_lines.append(process_name_string(line.strip()))
            # æ”¶è—é¢‘é“
            elif channel_name in favorite_dictionary and check_url_existence(favorite_lines, channel_address):
                favorite_lines.append(process_name_string(line.strip()))
            # æœªåŒ¹é…åˆ°ä»»ä½•åˆ†ç±»ï¼Œæ”¾å…¥other_lines
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)
                    other_lines.append(line.strip())

# è·å–éšæœºUser-Agentçš„å‡½æ•°
def get_random_user_agent():
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

# å¤„ç†å•ä¸ªURLçš„å‡½æ•°
def process_url(url):
    try:
        other_lines.append("â—†â—†â—†ã€€"+url)
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')
        with urllib.request.urlopen(req) as response:
            data = response.read()
            text = data.decode('utf-8')
            text = text.strip()
            is_m3u = text.startswith("#EXTM3U") or text.startswith("#EXTINF")
            if get_url_file_extension(url)==".m3u" or get_url_file_extension(url)==".m3u8" or is_m3u:
                text=convert_m3u_to_txt(text)
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for line in lines:
                if  "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    channel_name, channel_address = line.split(',', 1)
                    if "#" not in channel_address:
                        process_channel_line(line)
                    else:
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline=f'{channel_name},{channel_url}'
                            process_channel_line(newline)
            other_lines.append('\n')

    except Exception as e:
        print(f"âŒå¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# è·å–å½“å‰å·¥ä½œç›®å½•
current_directory = os.getcwd()

# ========= é¢‘é“å­—å…¸æ–‡ä»¶è¯»å– =========
# è¯»å–å„ç§é¢‘é“çš„å­—å…¸æ–‡ä»¶ï¼ˆç”¨äºé¢‘é“åˆ†ç±»ï¼‰
print(f"\nğŸ“‹ åŠ è½½é¢‘é“å­—å…¸...")

yangshi_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/CCTV.txt')
weishi_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/å«è§†.txt')

beijing_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/åŒ—äº¬.txt')
shanghai_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/ä¸Šæµ·.txt')
guangdong_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å¹¿ä¸œ.txt')
jiangsu_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ±Ÿè‹.txt')
zhejiang_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æµ™æ±Ÿ.txt')
shandong_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å±±ä¸œ.txt')
sichuan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å››å·.txt')
henan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ²³å—.txt')
hunan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ¹–å—.txt')
chongqing_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/é‡åº†.txt')
tianjin_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å¤©æ´¥.txt')
hubei_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ¹–åŒ—.txt')
anhui_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å®‰å¾½.txt')
fujian_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/ç¦å»º.txt')
liaoning_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/è¾½å®.txt')
shaanxi_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/é™•è¥¿.txt')
hebei_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ²³åŒ—.txt')
jiangxi_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ±Ÿè¥¿.txt')
guangxi_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å¹¿è¥¿.txt')
yunnan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/äº‘å—.txt')
shanxi_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å±±è¥¿.txt')
heilongjiang_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/é»‘é¾™æ±Ÿ.txt')
jilin_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å‰æ—.txt')
guizhou_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/è´µå·.txt')
gansu_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/ç”˜è‚ƒ.txt')
neimenggu_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å†…è’™.txt')
xinjiang_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ–°ç–†.txt')
hainan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æµ·å—.txt')
ningxia_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å®å¤.txt')
qinghai_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/é’æµ·.txt')
xizang_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/è¥¿è—.txt')

hongkong_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/é¦™æ¸¯.txt')
macau_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/æ¾³é—¨.txt')
taiwan_dictionary = read_txt_to_array('assets/livesource/åœ°æ–¹å°/å°æ¹¾.txt')

digital_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ•°å­—.txt')
movie_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ç”µå½±.txt')
tv_drama_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ç”µè§†å‰§.txt')
documentary_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/çºªå½•ç‰‡.txt')
cartoon_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt')
radio_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ”¶éŸ³æœº.txt')
variety_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ç»¼è‰º.txt')
huya_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/è™ç‰™.txt')
douyu_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ–—é±¼.txt')
commentary_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/è§£è¯´.txt')
music_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/éŸ³ä¹.txt')
food_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ç¾é£Ÿ.txt')
travel_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ—…æ¸¸.txt')
health_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/å¥åº·.txt')
finance_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/è´¢ç».txt')
shopping_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/è´­ç‰©.txt')
game_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ¸¸æˆ.txt')
news_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ–°é—».txt')
china_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ä¸­å›½.txt')
international_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/å›½é™….txt')
sports_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ä½“è‚².txt')
tyss_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt')
mgss_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/å’ªå’•èµ›äº‹.txt')
traditional_opera_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æˆæ›².txt')
spring_festival_gala_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ˜¥æ™š.txt')
camera_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')
favorite_dictionary = read_txt_to_array('assets/livesource/ä¸»é¢‘é“/æ”¶è—é¢‘é“.txt')

# æ‰“å°æ‰€æœ‰å­—å…¸åŠ è½½æƒ…å†µ
print(f"âœ… å­—å…¸åŠ è½½å®Œæˆ:")
print(f"   å¤®è§†: {len(yangshi_dictionary)} æ¡")
print(f"   å«è§†: {len(weishi_dictionary)} æ¡")
print(f"   åœ°æ–¹å°: 31ä¸ªåˆ†ç±»å·²åŠ è½½")
print(f"   æ¸¯æ¾³å°: 3ä¸ªåˆ†ç±»å·²åŠ è½½")
print(f"   å…¶ä»–åˆ†ç±»: 27ä¸ªåˆ†ç±»å·²åŠ è½½")

# åŠ è½½é¢‘é“åç§°ä¿®æ­£å­—å…¸çš„å‡½æ•°
def load_corrections_name(filename):
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œä»¥#å¼€å¤´çš„æ³¨é‡Šè¡Œ
                if not line or line.startswith('#'):
                    continue
                parts = line.strip().split(',')
                if len(parts) < 2:
                    continue  # è·³è¿‡ä¸å®Œæ•´çš„è¡Œ
                correct_name = parts[0]
                for name in parts[1:]:
                    if name:  # è·³è¿‡ç©ºçš„åˆ«å
                        corrections[name] = correct_name

    except FileNotFoundError:
        print(f"âŒ ä¿®æ­£å­—å…¸æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ åŠ è½½ä¿®æ­£å­—å…¸é”™è¯¯: {e}")
    return corrections

# åŠ è½½åç§°ä¿®æ­£å­—å…¸
corrections_name = load_corrections_name('assets/livesource/corrections_name.txt')
print(f"\nğŸ”„ é¢‘é“æ›´åä¿®æ­£å­—å…¸:")
print(f"   åŠ è½½äº† {len(corrections_name)} æ¡ä¿®æ­£è§„åˆ™")
if corrections_name:
    print(f"   ä¿®æ­£è§„åˆ™ç¤ºä¾‹ (å‰3æ¡):")
    for i, (wrong_name, correct_name) in enumerate(list(corrections_name.items())[:3]):
        print(f"     {i+1}. '{wrong_name}' â†’ '{correct_name}'")
    if len(corrections_name) > 3:
        print(f"     ... è¿˜æœ‰ {len(corrections_name) - 3} æ¡ä¿®æ­£è§„åˆ™")
else:
    print(f"   æœªåŠ è½½åˆ°æœ‰æ•ˆçš„ä¿®æ­£è§„åˆ™")

# ä¿®æ­£é¢‘é“åç§°æ•°æ®çš„å‡½æ•°
def correct_name_data(corrections, data):
    corrected_data = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            continue
        name, url = line.split(',', 1)
        if name in corrections and name != corrections[name]:
            name = corrections[name]
        corrected_data.append(f"{name},{url}")
    return corrected_data

# æŒ‰æŒ‡å®šé¡ºåºæ’åºæ•°æ®çš„å‡½æ•°
def sort_data(order, data):
    order_dict = {name: i for i, name in enumerate(order)}
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# è¯»å–URLåˆ—è¡¨æ–‡ä»¶
urls = read_txt_to_array('assets/livesource/urls-daily.txt')
print(f"\nğŸ“‹ å‘ç° {len(urls)} ä¸ªæ•°æ®è®¢é˜…æº")
for url in urls:
    if url.startswith("http"):
        if "{MMdd}" in url:
            current_date_str = datetime.now().strftime("%m%d")
            url=url.replace("{MMdd}", current_date_str)
        if "{MMdd-1}" in url:
            yesterday_date_str = (datetime.now() - timedelta(days=1)).strftime("%m%d")
            url=url.replace("{MMdd-1}", yesterday_date_str)
        print(f"ğŸ“¡ å¤„ç†URL: {url}")
        process_url(url)

# ä»å­—ç¬¦ä¸²ä¸­æå–æ•°å­—çš„å‡½æ•°ï¼ˆç”¨äºæ’åºï¼‰
def extract_number(s):
    num_str = s.split(',')[0].split('-')[1]
    numbers = re.findall(r'\d+', num_str)
    return int(numbers[-1]) if numbers else 999

# è‡ªå®šä¹‰æ’åºå‡½æ•°ï¼ˆä¼˜å…ˆ4Kã€8Ké¢‘é“ï¼‰
def custom_sort(s):
    if "CCTV-4K" in s:
        return 2
    elif "CCTV-8K" in s:
        return 3
    elif "(4K)" in s:
        return 1
    else:
        return 0

# å¤„ç†ç™½åå•è‡ªåŠ¨æ–‡ä»¶
print(f"\nğŸŸ¢ å¤„ç†ç™½åå•è‡ªåŠ¨æ–‡ä»¶...")
whitelist_auto_lines = read_txt_to_array('assets/livesource/blacklist/whitelist_auto.txt')

# æ‰“å°ç™½åå•ç»Ÿè®¡ä¿¡æ¯
print(f"   è¯»å–åˆ° {len(whitelist_auto_lines)} æ¡è®°å½•")

# ç»Ÿè®¡æœ‰æ•ˆç™½åå•è®°å½•
valid_whitelist_count = 0
valid_whitelist_samples = []

for i, whitelist_line in enumerate(whitelist_auto_lines):
    if i < 2:  # è·³è¿‡å‰ä¸¤è¡Œï¼ˆæ ‡é¢˜å’Œæ—¥æœŸè¡Œï¼‰
        continue
    # è·³è¿‡è¡¨å¤´è¡Œ
    if whitelist_line.startswith("RespoTime,whitelist,#genre#"):
        continue
    # å¤„ç†çœŸæ­£çš„ç™½åå•è¡Œ
    if "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
        whitelist_parts = whitelist_line.split(",")
        if len(whitelist_parts) >= 3:
            valid_whitelist_count += 1
            # ä¿å­˜ç¤ºä¾‹
            if len(valid_whitelist_samples) < 3:
                valid_whitelist_samples.append(whitelist_line)
            try:
                response_time = float(whitelist_parts[0].replace("ms", ""))
            except ValueError:
                print(f"response_timeè½¬æ¢å¤±è´¥: {whitelist_line}")
                response_time = 60000
            if response_time < 2000:
                process_channel_line(",".join(whitelist_parts[1:]))

print(f"   æœ‰æ•ˆç™½åå•è®°å½•: {valid_whitelist_count} æ¡")
if valid_whitelist_samples:
    print(f"   ç™½åå•ç¤ºä¾‹ (å‰3æ¡):")
    for i, line in enumerate(valid_whitelist_samples[:3]):
        print(f"     {i+1}. {line[:80]}..." if len(line) > 80 else f"     {i+1}. {line}")
    if valid_whitelist_count > 3:
        print(f"     ... è¿˜æœ‰ {valid_whitelist_count - 3} æ¡")

# éå†è‡ªåŠ¨æµ‹è¯•ç”Ÿæˆçš„æ’­æ”¾æºç™½åå•
for whitelist_line in whitelist_auto_lines:
    if  "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
        whitelist_parts = whitelist_line.split(",")
        try:
            response_time = float(whitelist_parts[0].replace("ms", ""))
        except ValueError:
            print(f"response_timeè½¬æ¢å¤±è´¥: {whitelist_line}")
            response_time = 60000
        if response_time < 2000:
            process_channel_line(",".join(whitelist_parts[1:]))

# è·å–HTTPå“åº”çš„å‡½æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
def get_http_response(url, timeout=8, retries=2, backoff_factor=1.0):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(retries):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                return data.decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"[HTTPError] Code: {e.code}, URL: {url}")
            break
        except urllib.error.URLError as e:
            print(f"[URLError] Reason: {e.reason}, Attempt: {attempt + 1}")
        except socket.timeout:
            print(f"[Timeout] URL: {url}, Attempt: {attempt + 1}")
        except Exception as e:
            print(f"[Exception] {type(e).__name__}: {e}, Attempt: {attempt + 1}")
            if attempt < retries - 1:
                time.sleep(backoff_factor * (2 ** attempt))
    return None

# å°†æ—¥æœŸæ ¼å¼è§„èŒƒåŒ–ä¸ºMM-DDæ ¼å¼çš„å‡½æ•°
def normalize_date_to_md(text):
    text = text.strip()
    def format_md(m):
        month = int(m.group(1))
        day = int(m.group(2))
        after = m.group(3) or ''
        if not after.startswith(' '):
            after = ' ' + after
        return f"{month:02d}-{day:02d}{after}"
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)
    return text

# è§„èŒƒåŒ–ä½“è‚²èµ›äº‹è¡Œçš„æ—¥æœŸæ ¼å¼
normalized_tyss_lines = [normalize_date_to_md(s) for s in tyss_lines]

# ========= AKTVç‰¹æ®Šå¤„ç† =========
aktv_lines = []  # å­˜å‚¨AKTVé¢‘é“æ•°æ®
aktv_url = "https://raw.githubusercontent.com/xiaoran67/update/refs/heads/main/assets/livesource/blacklist/whitelist_manual.txt"  # AKTVæºåœ°å€
aktv_text = get_http_response(aktv_url)
if aktv_text:
    print(f" ")
    print(f"ğŸ“º AKTVæˆåŠŸè·å–å†…å®¹")
    aktv_text = convert_m3u_to_txt(aktv_text)
    aktv_lines = aktv_text.strip().split('\n')
else:
    print(f"âš ï¸ AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
    aktv_lines = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/AKTV.txt')

print(f"AKTVé¢‘é“ç»Ÿè®¡:")
print(f"   è·å–åˆ° {len(aktv_lines)} æ¡AKTVé¢‘é“è®°å½•")
if aktv_lines:
    print(f"   AKTVé¢‘é“ç¤ºä¾‹ (å‰3æ¡):")
    for i, line in enumerate(aktv_lines[:3]):
        print(f"     {i+1}. {line[:60]}..." if len(line) > 60 else f"     {i+1}. {line}")
    if len(aktv_lines) > 3:
        print(f"     ... è¿˜æœ‰ {len(aktv_lines) - 3} æ¡")

# è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¡Œçš„å‡½æ•°
def filter_lines(lines, exclude_keywords):
    return [line for line in lines if not any(keyword in line for keyword in exclude_keywords)]

# ç”Ÿæˆä½“è‚²èµ›äº‹HTMLé¡µé¢çš„å‡½æ•°
def generate_playlist_html(data_list, output_file='playlist.html'):
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Setup Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''

    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''

    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… ç½‘é¡µå·²ç”Ÿæˆï¼š{output_file}")

# è‡ªå®šä¹‰ä½“è‚²èµ›äº‹æ’åºå‡½æ•°ï¼ˆæ•°å­—å‰ç¼€çš„å€’åºï¼Œå…¶ä»–æ­£åºï¼‰
def custom_tyss_sort(lines):
    digit_prefix = []
    others = []
    for line in lines:
        name_part = line.split(',')[0].strip()
        if name_part and name_part[0].isdigit():
            digit_prefix.append(line)
        else:
            others.append(line)
    digit_prefix_sorted = sorted(digit_prefix, reverse=True)
    others_sorted = sorted(others)

    return digit_prefix_sorted + others_sorted

# è¿‡æ»¤ä½“è‚²èµ›äº‹æ–‡æœ¬ä¸­çš„ç‰¹å®šå…³é”®è¯
keywords_to_exclude_tiyu_txt = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†","å…¬ä¼—å·","éº»è±†","ã€Œå›çœ‹ã€"]
normalized_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu_txt)

# å»é‡å¹¶æ’åº
normalized_tyss_lines = custom_tyss_sort(set(normalized_tyss_lines))

# è¿‡æ»¤ä½“è‚²èµ›äº‹HTMLä¸­çš„ç‰¹å®šå…³é”®è¯
keywords_to_exclude_tiyu = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†","å…¬ä¼—å·","å’ªè§†é€š","éº»è±†","ã€Œå›çœ‹ã€"]
filtered_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu)
print(f"\nğŸ† ä½“è‚²èµ›äº‹å¤„ç†å®Œæˆï¼šåŸå§‹ {len(tyss_lines)} æ¡ï¼Œè¿‡æ»¤å {len(filtered_tyss_lines)} æ¡")

# ç”Ÿæˆä½“è‚²èµ›äº‹HTMLæ–‡ä»¶
generate_playlist_html(filtered_tyss_lines, 'output/tiyu.html')

# ç”Ÿæˆä½“è‚²èµ›äº‹TXTæ–‡ä»¶
with open('output/tiyu.txt', 'w', encoding='utf-8') as f:
    for line in filtered_tyss_lines:
        f.write(line + '\n')
print(f"âœ… æ–‡æœ¬å·²ç”Ÿæˆ: output/tiyu.txt")

# ä»æ–‡ä»¶ä¸­éšæœºè·å–URLçš„å‡½æ•°
def get_random_url(file_path):
    urls = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            url = line.strip().split(',')[-1]
            urls.append(url)    
    return random.choice(urls) if urls else None

# ========= ä»Šæ—¥æ¨èå’Œç‰ˆæœ¬ä¿¡æ¯ =========
print(f"\nğŸ•’ ç”Ÿæˆä»Šæ—¥æ¨èå’Œç‰ˆæœ¬ä¿¡æ¯")
# è·å–åŒ—äº¬æ—¶é—´
utc_time = datetime.now(timezone.utc)
beijing_time = utc_time + timedelta(hours=8)
formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

# ç”Ÿæˆä»Šæ—¥æ¨èä¿¡æ¯
MTV1 = "ğŸ’¯æ¨è," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
MTV2 = "ğŸ¤«ä½è°ƒ," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
MTV3 = "ğŸŸ¢ä½¿ç”¨," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
MTV4 = "âš ï¸ç¦æ­¢," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
MTV5 = "ğŸš«è´©å–," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')

about_video1 = "https://gitee.com/xiaoran67/update/raw/master/assets/livesource/about1080p.mp4"
about_video2 = "https://gitlab.com/xiaoran67/update/-/raw/main/assets/livesource/about1080p.mp4"

# ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯
version = formatted_time + "," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')
about = "ğŸ‘¨æ½‡ç„¶," + get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')

# å¤„ç†æ‰‹å·¥æ·»åŠ çš„é¢‘é“æº
print(f"\nğŸ”§ å¤„ç†æ‰‹å·¥åŒºé«˜è´¨é‡æº...")

# è¯»å–å¹¶ç»Ÿè®¡å„ä¸ªæ‰‹å·¥åŒºæ–‡ä»¶
zhejiang_manual = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/æµ™æ±Ÿé¢‘é“.txt')
guangdong_manual = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/å¹¿ä¸œé¢‘é“.txt')
hubei_manual = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/æ¹–åŒ—é¢‘é“.txt')
shanghai_manual = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¸Šæµ·é¢‘é“.txt')
jiangsu_manual = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/æ±Ÿè‹é¢‘é“.txt')

# æ‰“å°æ‰‹å·¥åŒºç»Ÿè®¡ä¿¡æ¯
print(f"   æµ™æ±Ÿé¢‘é“: {len(zhejiang_manual)} æ¡")
print(f"   å¹¿ä¸œé¢‘é“: {len(guangdong_manual)} æ¡")
print(f"   æ¹–åŒ—é¢‘é“: {len(hubei_manual)} æ¡")
print(f"   ä¸Šæµ·é¢‘é“: {len(shanghai_manual)} æ¡")
print(f"   æ±Ÿè‹é¢‘é“: {len(jiangsu_manual)} æ¡")
print(f"   æ‰‹å·¥åŒºæ€»è®¡: {len(zhejiang_manual) + len(guangdong_manual) + len(hubei_manual) + len(shanghai_manual) + len(jiangsu_manual)} æ¡")

# æ·»åŠ åˆ°å¯¹åº”çš„é¢‘é“åˆ—è¡¨
zhejiang_lines += zhejiang_manual
guangdong_lines += guangdong_manual
hubei_lines += hubei_manual
shanghai_lines += shanghai_manual
jiangsu_lines += jiangsu_manual

print(f"\nğŸ“„ ç”Ÿæˆæ’­æ”¾åˆ—è¡¨æ–‡ä»¶")

# æ„å»ºå®Œæ•´ç‰ˆæ’­æ”¾åˆ—è¡¨
playlist_full =  ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary,correct_name_data(corrections_name,yangshi_lines)) + ['\n'] + \
        ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary,set(correct_name_data(corrections_name,weishi_lines))) + ['\n'] + \
        ["ğŸ›ï¸åŒ—äº¬é¢‘é“,#genre#"] + sort_data(beijing_dictionary,list(set(correct_name_data(corrections_name,beijing_lines)))) + ['\n'] + \
        ["ğŸ™ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(shanghai_dictionary,list(set(correct_name_data(corrections_name,shanghai_lines)))) + ['\n'] + \
        ["ğŸ¦å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(guangdong_dictionary,list(set(correct_name_data(corrections_name,guangdong_lines)))) + ['\n'] + \
        ["ğŸƒæ±Ÿè‹é¢‘é“,#genre#"] + sort_data(jiangsu_dictionary, list(set(correct_name_data(corrections_name, jiangsu_lines)))) + ['\n'] + \
        ["ğŸ§µæµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zhejiang_dictionary, list(set(correct_name_data(corrections_name, zhejiang_lines)))) + ['\n'] + \
        ["â›°ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary, list(set(correct_name_data(corrections_name, shandong_lines)))) + ['\n'] + \
        ["ğŸ¼å››å·é¢‘é“,#genre#"] + sort_data(sichuan_dictionary, list(set(correct_name_data(corrections_name, sichuan_lines)))) + ['\n'] + \
        ["âš”ï¸æ²³å—é¢‘é“,#genre#"] + sort_data(henan_dictionary, list(set(correct_name_data(corrections_name,henan_lines)))) + ['\n'] + \
        ["ğŸŒ¶ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(hunan_dictionary, list(set(correct_name_data(corrections_name,hunan_lines)))) + ['\n'] + \
        ["ğŸ²é‡åº†é¢‘é“,#genre#"] + sort_data(chongqing_dictionary, list(set(correct_name_data(corrections_name, chongqing_lines)))) + ['\n'] + \
        ["ğŸš¢å¤©æ´¥é¢‘é“,#genre#"] + sort_data(tianjin_dictionary, list(set(correct_name_data(corrections_name, tianjin_lines)))) + ['\n'] + \
        ["ğŸŒ‰æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hubei_dictionary, list(set(correct_name_data(corrections_name,hubei_lines)))) + ['\n'] + \
        ["ğŸŒ¾å®‰å¾½é¢‘é“,#genre#"] + sort_data(anhui_dictionary, list(set(correct_name_data(corrections_name, anhui_lines)))) + ['\n'] + \
        ["ğŸŒŠç¦å»ºé¢‘é“,#genre#"] + sort_data(fujian_dictionary, list(set(correct_name_data(corrections_name, fujian_lines)))) + ['\n'] + \
        ["ğŸ­è¾½å®é¢‘é“,#genre#"] + sort_data(liaoning_dictionary, list(set(correct_name_data(corrections_name, liaoning_lines)))) + ['\n'] + \
        ["ğŸ—¿é™•è¥¿é¢‘é“,#genre#"] + sort_data(shaanxi_dictionary, list(set(correct_name_data(corrections_name, shaanxi_lines)))) + ['\n'] + \
        ["â›©ï¸æ²³åŒ—é¢‘é“,#genre#"] + sort_data(hebei_dictionary, list(set(correct_name_data(corrections_name, hebei_lines)))) + ['\n'] + \
        ["ğŸ¶æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(jiangxi_dictionary, list(set(correct_name_data(corrections_name, jiangxi_lines)))) + ['\n'] + \
        ["ğŸ’ƒå¹¿è¥¿é¢‘é“,#genre#"] + sort_data(guangxi_dictionary,list(set(correct_name_data(corrections_name,guangxi_lines)))) + ['\n'] + \
        ["â˜ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(yunnan_dictionary, list(set(correct_name_data(corrections_name, yunnan_lines)))) + ['\n'] + \
        ["ğŸ®å±±è¥¿é¢‘é“,#genre#"] + sort_data(shanxi_dictionary, list(set(correct_name_data(corrections_name, shanxi_lines)))) + ['\n'] + \
        ["â„ï¸é»‘Â·é¾™Â·æ±Ÿ,#genre#"] + sort_data(heilongjiang_dictionary, list(set(correct_name_data(corrections_name, heilongjiang_lines)))) + ['\n'] + \
        ["ğŸå‰æ—é¢‘é“,#genre#"] + sort_data(jilin_dictionary, list(set(correct_name_data(corrections_name, jilin_lines)))) + ['\n'] + \
        ["ğŸŒˆè´µå·é¢‘é“,#genre#"] + sort_data(guizhou_dictionary, list(set(correct_name_data(corrections_name, guizhou_lines)))) + ['\n'] + \
        ["ğŸ«ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(gansu_dictionary, list(set(correct_name_data(corrections_name, gansu_lines)))) + ['\n'] + \
        ["ğŸå†…Â·è’™Â·å¤,#genre#"] + sort_data(neimenggu_dictionary, list(set(correct_name_data(corrections_name, neimenggu_lines)))) + ['\n'] + \
        ["ğŸ‡æ–°ç–†é¢‘é“,#genre#"] + sort_data(xinjiang_dictionary, list(set(correct_name_data(corrections_name, xinjiang_lines)))) + ['\n'] + \
        ["ğŸï¸æµ·å—é¢‘é“,#genre#"] + sort_data(hainan_dictionary, list(set(correct_name_data(corrections_name, hainan_lines)))) + ['\n'] + \
        ["ğŸ•Œå®å¤é¢‘é“,#genre#"] + sort_data(ningxia_dictionary, list(set(correct_name_data(corrections_name, ningxia_lines)))) + ['\n'] + \
        ["ğŸ‘é’æµ·é¢‘é“,#genre#"] + sort_data(qinghai_dictionary, list(set(correct_name_data(corrections_name, qinghai_lines)))) + ['\n'] + \
        ["ğŸè¥¿è—é¢‘é“,#genre#"] + sort_data(xizang_dictionary, list(set(correct_name_data(corrections_name, xizang_lines)))) + ['\n'] + \
        ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(hongkong_dictionary, list(set(correct_name_data(corrections_name, hongkong_lines)))) + ['\n'] + \
        ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(macau_dictionary, list(set(correct_name_data(corrections_name, macau_lines)))) + ['\n'] + \
        ["ğŸ‡¨ğŸ‡³å°æ¹¾é¢‘é“,#genre#"] + sort_data(taiwan_dictionary, list(set(correct_name_data(corrections_name, taiwan_lines)))) + ['\n'] + \
        ["ğŸ‡¨ğŸ‡³ä¸­å›½ç»¼åˆ,#genre#"] + sort_data(china_dictionary, list(set(correct_name_data(corrections_name, china_lines)))) + ['\n'] + \
        ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(international_dictionary, list(set(correct_name_data(corrections_name, international_lines)))) + ['\n'] + \
        ["ğŸ“¶æ•°å­—é¢‘é“,#genre#"] + sort_data(digital_dictionary, list(set(correct_name_data(corrections_name, digital_lines)))) + ['\n'] + \
        ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(movie_dictionary, list(set(correct_name_data(corrections_name, movie_lines)))) + ['\n'] + \
        ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(tv_drama_dictionary, list(set(correct_name_data(corrections_name, tv_drama_lines)))) + ['\n'] + \
        ["ğŸ¦ŠåŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(cartoon_dictionary, list(set(correct_name_data(corrections_name, cartoon_lines)))) + ['\n'] + \
        ["ğŸ“½ï¸çºªÂ·å½•Â·ç‰‡,#genre#"] + sort_data(documentary_dictionary, list(set(correct_name_data(corrections_name, documentary_lines)))) + ['\n'] + \
        ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, list(set(correct_name_data(corrections_name, radio_lines)))) + ['\n'] + \
        ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, list(set(correct_name_data(corrections_name, huya_lines)))) + ['\n'] + \
        ["ğŸ æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, list(set(correct_name_data(corrections_name, douyu_lines)))) + ['\n'] + \
        ["ğŸ¤è§£è¯´é¢‘é“,#genre#"] + sort_data(commentary_dictionary, list(set(correct_name_data(corrections_name, commentary_lines)))) + ['\n'] + \
        ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(music_dictionary, list(set(correct_name_data(corrections_name, music_lines)))) + ['\n'] + \
        ["ğŸœç¾é£Ÿé¢‘é“,#genre#"] + sort_data(food_dictionary, list(set(correct_name_data(corrections_name, food_lines)))) + ['\n'] + \
        ["âœˆï¸æ—…æ¸¸é¢‘é“,#genre#"] + sort_data(travel_dictionary, list(set(correct_name_data(corrections_name, travel_lines)))) + ['\n'] + \
        ["ğŸ¥å¥åº·é¢‘é“,#genre#"] + sort_data(health_dictionary, list(set(correct_name_data(corrections_name, health_lines)))) + ['\n'] + \
        ["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(news_dictionary, list(set(correct_name_data(corrections_name, news_lines)))) + ['\n'] + \
        ["ğŸ’°è´¢ç»é¢‘é“,#genre#"] + sort_data(finance_dictionary, list(set(correct_name_data(corrections_name, finance_lines)))) + ['\n'] + \
        ["ğŸ›ï¸è´­ç‰©é¢‘é“,#genre#"] + sort_data(shopping_dictionary, list(set(correct_name_data(corrections_name, shopping_lines)))) + ['\n'] + \
        ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sort_data(game_dictionary,set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
        ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, traditional_opera_lines))) + ['\n'] + \
        ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, variety_lines))) + ['\n'] + \
        ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(spring_festival_gala_dictionary,list(set(spring_festival_gala_lines)))  + ['\n'] + \
        ["â­æ”¶è—é¢‘é“,#genre#"] + sort_data(favorite_dictionary, list(set(correct_name_data(corrections_name, favorite_lines)))) + ['\n'] + \
        ["âš½ï¸ä½“è‚²é¢‘é“,#genre#"] + sort_data(sports_dictionary,set(correct_name_data(corrections_name,sports_lines))) + ['\n'] + \
        ["ğŸ†ï¸ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
        ["ğŸˆå’ªå’•èµ›äº‹,#genre#"] + mgss_lines + ['\n'] + \
        ["ğŸ‘‘ä¸“äº«å¤®è§†,#genre#"] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
        ["â˜•ï¸ä¸“äº«å«è§†,#genre#"] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å«è§†.txt') + ['\n'] + \
        ["ğŸï¸æ™¯åŒºç›´æ’­,#genre#"] + sorted(set(correct_name_data(corrections_name,camera_lines))) + ['\n'] + \
        ["ğŸ“¦å…¶ä»–é¢‘é“,#genre#"] + sorted(set(other_lines)) + ['\n'] + \
        ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [MTV1] + [MTV2] + [MTV3] + [MTV4] + [MTV5] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/about.txt') + ['\n']

# æ„å»ºç²¾ç®€ç‰ˆæ’­æ”¾åˆ—è¡¨
playlist_lite =  ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary,correct_name_data(corrections_name,yangshi_lines)) + ['\n'] + \
        ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary,set(correct_name_data(corrections_name,weishi_lines))) + ['\n'] + \
        ["ğŸ åœ°Â·æ–¹Â·å°,#genre#"] + \
        sort_data(beijing_dictionary, list(set(correct_name_data(corrections_name, beijing_lines)))) + \
        sort_data(shanghai_dictionary, list(set(correct_name_data(corrections_name, shanghai_lines)))) + \
        sort_data(guangdong_dictionary, list(set(correct_name_data(corrections_name, guangdong_lines)))) + \
        sort_data(jiangsu_dictionary, list(set(correct_name_data(corrections_name, jiangsu_lines)))) + \
        sort_data(zhejiang_dictionary, list(set(correct_name_data(corrections_name, zhejiang_lines)))) + \
        sort_data(shandong_dictionary, list(set(correct_name_data(corrections_name, shandong_lines)))) + \
        sort_data(sichuan_dictionary, list(set(correct_name_data(corrections_name, sichuan_lines)))) + \
        sort_data(henan_dictionary, list(set(correct_name_data(corrections_name,henan_lines)))) + \
        sort_data(hunan_dictionary, list(set(correct_name_data(corrections_name,hunan_lines)))) + \
        sort_data(chongqing_dictionary, list(set(correct_name_data(corrections_name, chongqing_lines)))) + \
        sort_data(tianjin_dictionary, list(set(correct_name_data(corrections_name, tianjin_lines)))) + \
        sort_data(hubei_dictionary, list(set(correct_name_data(corrections_name,hubei_lines)))) + \
        sort_data(anhui_dictionary, list(set(correct_name_data(corrections_name, anhui_lines)))) + \
        sort_data(fujian_dictionary, list(set(correct_name_data(corrections_name, fujian_lines)))) + \
        sort_data(liaoning_dictionary, list(set(correct_name_data(corrections_name, liaoning_lines)))) + \
        sort_data(shaanxi_dictionary, list(set(correct_name_data(corrections_name, shaanxi_lines)))) + \
        sort_data(hebei_dictionary, list(set(correct_name_data(corrections_name, hebei_lines)))) + \
        sort_data(jiangxi_dictionary, list(set(correct_name_data(corrections_name, jiangxi_lines)))) + \
        sort_data(guangxi_dictionary,list(set(correct_name_data(corrections_name,guangxi_lines)))) + \
        sort_data(yunnan_dictionary, list(set(correct_name_data(corrections_name, yunnan_lines)))) + \
        sort_data(shanxi_dictionary, list(set(correct_name_data(corrections_name, shanxi_lines)))) + \
        sort_data(heilongjiang_dictionary, list(set(correct_name_data(corrections_name, heilongjiang_lines)))) + \
        sort_data(jilin_dictionary, list(set(correct_name_data(corrections_name, jilin_lines)))) + \
        sort_data(guizhou_dictionary, list(set(correct_name_data(corrections_name, guizhou_lines)))) + \
        sort_data(gansu_dictionary, list(set(correct_name_data(corrections_name, gansu_lines)))) + \
        sort_data(neimenggu_dictionary, list(set(correct_name_data(corrections_name, neimenggu_lines)))) + \
        sort_data(xinjiang_dictionary, list(set(correct_name_data(corrections_name, xinjiang_lines)))) + \
        sort_data(hainan_dictionary, list(set(correct_name_data(corrections_name, hainan_lines)))) + \
        sort_data(ningxia_dictionary, list(set(correct_name_data(corrections_name, ningxia_lines)))) + \
        sort_data(qinghai_dictionary, list(set(correct_name_data(corrections_name, qinghai_lines)))) + \
        sort_data(xizang_dictionary, list(set(correct_name_data(corrections_name, xizang_lines)))) + \
        ['\n'] + \
        ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [MTV1] + [MTV2] + [MTV3] + [MTV4] + [MTV5] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/about.txt') + ['\n']

# æ„å»ºå®šåˆ¶ç‰ˆæ’­æ”¾åˆ—è¡¨
playlist_custom = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary, correct_name_data(corrections_name,yangshi_lines)) + ['\n'] + \
        ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary,set(correct_name_data(corrections_name,weishi_lines))) + ['\n'] + \
        ["ğŸ åœ°Â·æ–¹Â·å°,#genre#"] + \
        sort_data(beijing_dictionary, list(set(correct_name_data(corrections_name, beijing_lines)))) + \
        sort_data(shanghai_dictionary, list(set(correct_name_data(corrections_name, shanghai_lines)))) + \
        sort_data(guangdong_dictionary, list(set(correct_name_data(corrections_name, guangdong_lines)))) + \
        sort_data(jiangsu_dictionary, list(set(correct_name_data(corrections_name, jiangsu_lines)))) + \
        sort_data(zhejiang_dictionary, list(set(correct_name_data(corrections_name, zhejiang_lines)))) + \
        sort_data(shandong_dictionary, list(set(correct_name_data(corrections_name, shandong_lines)))) + \
        sort_data(sichuan_dictionary, list(set(correct_name_data(corrections_name, sichuan_lines)))) + \
        sort_data(henan_dictionary, list(set(correct_name_data(corrections_name,henan_lines)))) + \
        sort_data(hunan_dictionary, list(set(correct_name_data(corrections_name,hunan_lines)))) + \
        sort_data(chongqing_dictionary, list(set(correct_name_data(corrections_name, chongqing_lines)))) + \
        sort_data(tianjin_dictionary, list(set(correct_name_data(corrections_name, tianjin_lines)))) + \
        sort_data(hubei_dictionary, list(set(correct_name_data(corrections_name,hubei_lines)))) + \
        sort_data(anhui_dictionary, list(set(correct_name_data(corrections_name, anhui_lines)))) + \
        sort_data(fujian_dictionary, list(set(correct_name_data(corrections_name, fujian_lines)))) + \
        sort_data(liaoning_dictionary, list(set(correct_name_data(corrections_name, liaoning_lines)))) + \
        sort_data(shaanxi_dictionary, list(set(correct_name_data(corrections_name, shaanxi_lines)))) + \
        sort_data(hebei_dictionary, list(set(correct_name_data(corrections_name, hebei_lines)))) + \
        sort_data(jiangxi_dictionary, list(set(correct_name_data(corrections_name, jiangxi_lines)))) + \
        sort_data(guangxi_dictionary,list(set(correct_name_data(corrections_name,guangxi_lines)))) + \
        sort_data(yunnan_dictionary, list(set(correct_name_data(corrections_name, yunnan_lines)))) + \
        sort_data(shanxi_dictionary, list(set(correct_name_data(corrections_name, shanxi_lines)))) + \
        sort_data(heilongjiang_dictionary, list(set(correct_name_data(corrections_name, heilongjiang_lines)))) + \
        sort_data(jilin_dictionary, list(set(correct_name_data(corrections_name, jilin_lines)))) + \
        sort_data(guizhou_dictionary, list(set(correct_name_data(corrections_name, guizhou_lines)))) + \
        sort_data(gansu_dictionary, list(set(correct_name_data(corrections_name, gansu_lines)))) + \
        sort_data(neimenggu_dictionary, list(set(correct_name_data(corrections_name, neimenggu_lines)))) + \
        sort_data(xinjiang_dictionary, list(set(correct_name_data(corrections_name, xinjiang_lines)))) + \
        sort_data(hainan_dictionary, list(set(correct_name_data(corrections_name, hainan_lines)))) + \
        sort_data(ningxia_dictionary, list(set(correct_name_data(corrections_name, ningxia_lines)))) + \
        sort_data(qinghai_dictionary, list(set(correct_name_data(corrections_name, qinghai_lines)))) + \
        sort_data(xizang_dictionary, list(set(correct_name_data(corrections_name, xizang_lines)))) + \
        ['\n'] + \
        ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(hongkong_dictionary, list(set(correct_name_data(corrections_name, hongkong_lines)))) + ['\n'] + \
        ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(macau_dictionary, list(set(correct_name_data(corrections_name, macau_lines)))) + ['\n'] + \
        ["ğŸ‡¨ğŸ‡³å°æ¹¾é¢‘é“,#genre#"] + sort_data(taiwan_dictionary, list(set(correct_name_data(corrections_name, taiwan_lines)))) + ['\n'] + \
        ["ğŸ‡¨ğŸ‡³ä¸­å›½ç»¼åˆ,#genre#"] + sort_data(china_dictionary, list(set(correct_name_data(corrections_name, china_lines)))) + ['\n'] + \
        ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(international_dictionary, list(set(correct_name_data(corrections_name, international_lines)))) + ['\n'] + \
        ["ğŸ“¶æ•°å­—é¢‘é“,#genre#"] + sort_data(digital_dictionary, list(set(correct_name_data(corrections_name, digital_lines)))) + ['\n'] + \
        ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(movie_dictionary, list(set(correct_name_data(corrections_name, movie_lines)))) + ['\n'] + \
        ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(tv_drama_dictionary, list(set(correct_name_data(corrections_name, tv_drama_lines)))) + ['\n'] + \
        ["ğŸ¦ŠåŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(cartoon_dictionary, list(set(correct_name_data(corrections_name, cartoon_lines)))) + ['\n'] + \
        ["ğŸ“½ï¸çºªÂ·å½•Â·ç‰‡,#genre#"] + sort_data(documentary_dictionary, list(set(correct_name_data(corrections_name, documentary_lines)))) + ['\n'] + \
        ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, list(set(correct_name_data(corrections_name, radio_lines)))) + ['\n'] + \
        ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, list(set(correct_name_data(corrections_name, huya_lines)))) + ['\n'] + \
        ["ğŸ æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, list(set(correct_name_data(corrections_name, douyu_lines)))) + ['\n'] + \
        ["ğŸ¤è§£è¯´é¢‘é“,#genre#"] + sort_data(commentary_dictionary, list(set(correct_name_data(corrections_name, commentary_lines)))) + ['\n'] + \
        ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(music_dictionary, list(set(correct_name_data(corrections_name, music_lines)))) + ['\n'] + \
        ["ğŸœç¾é£Ÿé¢‘é“,#genre#"] + sort_data(food_dictionary, list(set(correct_name_data(corrections_name, food_lines)))) + ['\n'] + \
        ["âœˆï¸æ—…æ¸¸é¢‘é“,#genre#"] + sort_data(travel_dictionary, list(set(correct_name_data(corrections_name, travel_lines)))) + ['\n'] + \
        ["ğŸ¥å¥åº·é¢‘é“,#genre#"] + sort_data(health_dictionary, list(set(correct_name_data(corrections_name, health_lines)))) + ['\n'] + \
        ["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(news_dictionary, list(set(correct_name_data(corrections_name, news_lines)))) + ['\n'] + \
        ["ğŸ’°è´¢ç»é¢‘é“,#genre#"] + sort_data(finance_dictionary, list(set(correct_name_data(corrections_name, finance_lines)))) + ['\n'] + \
        ["ğŸ›ï¸è´­ç‰©é¢‘é“,#genre#"] + sort_data(shopping_dictionary, list(set(correct_name_data(corrections_name, shopping_lines)))) + ['\n'] + \
        ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sort_data(game_dictionary,set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
        ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, traditional_opera_lines))) + ['\n'] + \
        ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, variety_lines))) + ['\n'] + \
        ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(spring_festival_gala_dictionary,list(set(spring_festival_gala_lines)))  + ['\n'] + \
        ["â­æ”¶è—é¢‘é“,#genre#"] + sort_data(favorite_dictionary, list(set(correct_name_data(corrections_name, favorite_lines)))) + ['\n'] + \
        ["âš½ï¸ä½“è‚²é¢‘é“,#genre#"] + sort_data(sports_dictionary,set(correct_name_data(corrections_name,sports_lines))) + ['\n'] + \
        ["ğŸ†ï¸ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
        ["ğŸˆå’ªå’•èµ›äº‹,#genre#"] + mgss_lines + ['\n'] + \
        ["ğŸ‘‘ä¸“äº«å¤®è§†,#genre#"] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
        ["â˜•ï¸ä¸“äº«å«è§†,#genre#"] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å«è§†.txt') + ['\n'] + \
        ["ğŸï¸æ™¯åŒºç›´æ’­,#genre#"] + sorted(set(correct_name_data(corrections_name,camera_lines))) + ['\n'] + \
        ["ğŸ“¦å…¶ä»–é¢‘é“,#genre#"] + sorted(set(other_lines)) + ['\n'] + \
        ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [MTV1] + [MTV2] + [MTV3] + [MTV4] + [MTV5] + read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/about.txt') + ['\n']

# å®šä¹‰è¾“å‡ºæ–‡ä»¶å
output_others = "output/others.txt"
output_full = "output/full.txt"
output_lite = "output/lite.txt"
output_custom = "output/custom.txt"

# å†™å…¥æ–‡ä»¶
try:
    with open(output_full, 'w', encoding='utf-8') as f:
        for line in playlist_full:
            f.write(line + '\n')
    print(f"âœ… å®Œæ•´ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_full}")

    with open(output_lite, 'w', encoding='utf-8') as f:
        for line in playlist_lite:
            f.write(line + '\n')
    print(f"âœ… ç²¾ç®€ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_lite}")

    with open(output_custom, 'w', encoding='utf-8') as f:
        for line in playlist_custom:
            f.write(line + '\n')
    print(f"âœ… å®šåˆ¶ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_custom}")

    with open(output_others, 'w', encoding='utf-8') as f:
        for line in other_lines:
            f.write(line + '\n')
    print(f"âœ… æœªåˆ†ç±»é¢‘é“åˆ—è¡¨å·²ä¿å­˜: {output_others}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# è¯»å–é¢‘é“Logoä¿¡æ¯
channels_logos=read_txt_to_array('assets/livesource/logo.txt')

# æ ¹æ®é¢‘é“åç§°è·å–Logo URLçš„å‡½æ•°
def get_logo_by_channel_name(channel_name):
    for line in channels_logos:
        if not line.strip():
            continue
        name, url = line.split(',')
        if name == channel_name:
            return url
    return None

# å°†TXTæ–‡ä»¶è½¬æ¢ä¸ºM3Uæ ¼å¼çš„å‡½æ•°
def make_m3u(txt_file, m3u_file):
    try:
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()
        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url=get_logo_by_channel_name(channel_name)
                if logo_url is None:
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)

        print(f"â–¶ï¸ M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

# ç”ŸæˆM3Uæ–‡ä»¶
make_m3u(output_full, output_full.replace(".txt", ".m3u"))
make_m3u(output_lite, output_lite.replace(".txt", ".m3u"))
make_m3u(output_custom, output_custom.replace(".txt", ".m3u"))

# ========= ç»Ÿè®¡ä¿¡æ¯ =========

# è®¡ç®—æ‰§è¡Œæ—¶é—´
print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡")
timeend = datetime.now()
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)
print(f"   å¼€å§‹æ—¶é—´: {timestart.strftime('%Y%m%d %H:%M:%S')}")
print(f"   ç»“æŸæ—¶é—´: {timeend.strftime('%Y%m%d %H:%M:%S')}")
print(f"   æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

# é¢‘é“æ•°æ®ç»Ÿè®¡
print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡")
print(f"   é»‘åå•æ¡æ•°: {len(combined_blacklist)}")
print(f"   å…¶ä»–æºæ¡æ•°: {len(other_lines)}")
print(f"   å®Œæ•´ç‰ˆæ¡æ•°: {len(playlist_full)}")
print(f"   ç²¾ç®€ç‰ˆæ¡æ•°: {len(playlist_lite)}")
print(f"   å®šåˆ¶ç‰ˆæ¡æ•°: {len(playlist_custom)}")

# é¢‘é“åˆ†ç±»ç»Ÿè®¡
print(f"\nğŸ“ åˆ†ç±»ç»Ÿè®¡:")

print(f"ğŸ“º ä¸»Â·é¢‘Â·é“")
print(f"   ğŸŒ å¤®è§†é¢‘é“: {len(yangshi_lines)}")
print(f"   ğŸ“¡ å«è§†é¢‘é“: {len(weishi_lines)}")

print(f"ğŸ  åœ°Â·æ–¹Â·å°")
print(f"   ğŸ›ï¸ åŒ—äº¬é¢‘é“: {len(beijing_lines)}")
print(f"   ğŸ™ï¸ ä¸Šæµ·é¢‘é“: {len(shanghai_lines)}")
print(f"   ğŸ¦ å¹¿ä¸œé¢‘é“: {len(guangdong_lines)}")
print(f"   ğŸƒ æ±Ÿè‹é¢‘é“: {len(jiangsu_lines)}")
print(f"   ğŸ§µ æµ™æ±Ÿé¢‘é“: {len(zhejiang_lines)}")
print(f"   â›°ï¸ å±±ä¸œé¢‘é“: {len(shandong_lines)}")
print(f"   ğŸ¼ å››å·é¢‘é“: {len(sichuan_lines)}")
print(f"   âš”ï¸ æ²³å—é¢‘é“: {len(henan_lines)}")
print(f"   ğŸŒ¶ï¸ æ¹–å—é¢‘é“: {len(hunan_lines)}")
print(f"   ğŸ² é‡åº†é¢‘é“: {len(chongqing_lines)}")
print(f"   ğŸš¢ å¤©æ´¥é¢‘é“: {len(tianjin_lines)}")
print(f"   ğŸŒ‰ æ¹–åŒ—é¢‘é“: {len(hubei_lines)}")
print(f"   ğŸŒ¾ å®‰å¾½é¢‘é“: {len(anhui_lines)}")
print(f"   ğŸŒŠ ç¦å»ºé¢‘é“: {len(fujian_lines)}")
print(f"   ğŸ­ è¾½å®é¢‘é“: {len(liaoning_lines)}")
print(f"   ğŸ—¿ é™•è¥¿é¢‘é“: {len(shaanxi_lines)}")
print(f"   â›©ï¸ æ²³åŒ—é¢‘é“: {len(hebei_lines)}")
print(f"   ğŸ¶ æ±Ÿè¥¿é¢‘é“: {len(jiangxi_lines)}")
print(f"   ğŸ’ƒ å¹¿è¥¿é¢‘é“: {len(guangxi_lines)}")
print(f"   â˜ï¸ äº‘å—é¢‘é“: {len(yunnan_lines)}")
print(f"   ğŸ® å±±è¥¿é¢‘é“: {len(shanxi_lines)}")
print(f"   â„ï¸ é»‘Â·é¾™Â·æ±Ÿ: {len(heilongjiang_lines)}")
print(f"   ğŸ å‰æ—é¢‘é“: {len(jilin_lines)}")
print(f"   ğŸŒˆ è´µå·é¢‘é“: {len(guizhou_lines)}")
print(f"   ğŸ« ç”˜è‚ƒé¢‘é“: {len(gansu_lines)}")
print(f"   ğŸ å†…Â·è’™Â·å¤: {len(neimenggu_lines)}")
print(f"   ğŸ‡ æ–°ç–†é¢‘é“: {len(xinjiang_lines)}")
print(f"   ğŸï¸ æµ·å—é¢‘é“: {len(hainan_lines)}")
print(f"   ğŸ•Œ å®å¤é¢‘é“: {len(ningxia_lines)}")
print(f"   ğŸ‘ é’æµ·é¢‘é“: {len(qinghai_lines)}")
print(f"   ğŸ è¥¿è—é¢‘é“: {len(xizang_lines)}")

print(f"ğŸ‡­ğŸ‡° æ¸¯Â·æ¾³Â·å°")
print(f"   ğŸ‡­ğŸ‡° é¦™æ¸¯é¢‘é“: {len(hongkong_lines)}")
print(f"   ğŸ‡²ğŸ‡´ æ¾³é—¨é¢‘é“: {len(macau_lines)}")
print(f"   ğŸ‡¨ğŸ‡³ å°æ¹¾é¢‘é“: {len(taiwan_lines)}")

print(f"ğŸ‘‘ å®šÂ·åˆ¶Â·å°")
print(f"   ğŸ“¶ æ•°å­—é¢‘é“: {len(digital_lines)}")
print(f"   ğŸ¬ ç”µå½±é¢‘é“: {len(movie_lines)}")
print(f"   ğŸ“º ç”µÂ·è§†Â·å‰§: {len(tv_drama_lines)}")
print(f"   ğŸ“½ï¸ çºªÂ·å½•Â·ç‰‡: {len(documentary_lines)}")
print(f"   ğŸ¦Š åŠ¨Â·ç”»Â·ç‰‡: {len(cartoon_lines)}")
print(f"   ğŸ“» æ”¶Â·éŸ³Â·æœº: {len(radio_lines)}")
print(f"   ğŸ­ ç»¼è‰ºé¢‘é“: {len(variety_lines)}")
print(f"   ğŸ¯ è™ç‰™é¢‘é“: {len(huya_lines)}")
print(f"   ğŸ  æ–—é±¼é¢‘é“: {len(douyu_lines)}")
print(f"   ğŸ¤ è§£è¯´é¢‘é“: {len(commentary_lines)}")
print(f"   ğŸµ éŸ³ä¹é¢‘é“: {len(music_lines)}")
print(f"   ğŸœ ç¾é£Ÿé¢‘é“: {len(food_lines)}")
print(f"   âœˆï¸ æ—…æ¸¸é¢‘é“: {len(travel_lines)}")
print(f"   ğŸ¥ å¥åº·é¢‘é“: {len(health_lines)}")
print(f"   ğŸ’° è´¢ç»é¢‘é“: {len(finance_lines)}")
print(f"   ğŸ›ï¸ è´­ç‰©é¢‘é“: {len(shopping_lines)}")
print(f"   ğŸ® æ¸¸æˆé¢‘é“: {len(game_lines)}")
print(f"   ğŸ“° æ–°é—»é¢‘é“: {len(news_lines)}")
print(f"   ğŸ‡¨ğŸ‡³ ä¸­å›½é¢‘é“: {len(china_lines)}")
print(f"   ğŸŒ å›½é™…é¢‘é“: {len(international_lines)}")
print(f"   âš½ï¸ ä½“è‚²é¢‘é“: {len(sports_lines)}")
print(f"   ğŸ†ï¸ ä½“è‚²èµ›äº‹: {len(filtered_tyss_lines)}")
print(f"   ğŸˆ å’ªå’•èµ›äº‹: {len(mgss_lines)}")
print(f"   ğŸ­ æˆæ›²é¢‘é“: {len(traditional_opera_lines)}")
print(f"   ğŸ§¨ å†å±Šæ˜¥æ™š: {len(spring_festival_gala_lines)}")
print(f"   ğŸï¸ æ™¯åŒºç›´æ’­: {len(camera_lines)}")
print(f"   â­ æ”¶è—é¢‘é“: {len(favorite_lines)}")

print(f"\nğŸ“¦ å…¶ä»–æœªåˆ†ç±»: {len(other_lines)}")

print("\nğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ!âœ…ğŸš€")

# ====== ç›´æ’­æºèšåˆå¤„ç†å·¥å…· v1.00 ======
