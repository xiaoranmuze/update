#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ====== ç›´æ’­æºèšåˆå¤„ç†å·¥å…· v3.02 ======
# ======= LiveSource-Collector =======
# ========= åŸºäºv2.00ï¼Œé‡æ„ç‰ˆ =========

# ========= æ¨¡å—å¯¼å…¥åŒº =========
import urllib.request
from urllib.parse import urlparse
import re  # æ­£åˆ™
import os
from datetime import datetime, timedelta, timezone
import random
import opencc  # ç®€ç¹è½¬æ¢
import socket
import time

# ========= åˆå§‹åŒ–è¾“å‡ºç›®å½• =========
os.makedirs('output', exist_ok=True)  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸ä¼šæŠ¥é”™
print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: output")

# ========= é¢‘é“åˆ†ç±»é…ç½® =========
# åŸºäºv2.00çš„æ‰€æœ‰åˆ†ç±»ï¼Œé‡æ„çš„ç»“æ„åŒ–é…ç½®æ–¹å¼
CHANNEL_CONFIG = {
    # ä¸»é¢‘é“
    "yangshi": {"lines": [], "title": "ğŸŒå¤®è§†é¢‘é“", "file": "ä¸»é¢‘é“/CCTV.txt"},
    "weishi": {"lines": [], "title": "ğŸ“¡å«è§†é¢‘é“", "file": "ä¸»é¢‘é“/å«è§†.txt"},
    
    # åœ°æ–¹å°ï¼ˆçœçº§ï¼‰
    "beijing": {"lines": [], "title": "ğŸ›ï¸åŒ—äº¬é¢‘é“", "file": "åœ°æ–¹å°/åŒ—äº¬.txt"},
    "shanghai": {"lines": [], "title": "ğŸ™ï¸ä¸Šæµ·é¢‘é“", "file": "åœ°æ–¹å°/ä¸Šæµ·.txt"},
    "guangdong": {"lines": [], "title": "ğŸ¦å¹¿ä¸œé¢‘é“", "file": "åœ°æ–¹å°/å¹¿ä¸œ.txt"},
    "jiangsu": {"lines": [], "title": "ğŸƒæ±Ÿè‹é¢‘é“", "file": "åœ°æ–¹å°/æ±Ÿè‹.txt"},
    "zhejiang": {"lines": [], "title": "ğŸ§µæµ™æ±Ÿé¢‘é“", "file": "åœ°æ–¹å°/æµ™æ±Ÿ.txt"},
    "shandong": {"lines": [], "title": "â›°ï¸å±±ä¸œé¢‘é“", "file": "åœ°æ–¹å°/å±±ä¸œ.txt"},
    "sichuan": {"lines": [], "title": "ğŸ¼å››å·é¢‘é“", "file": "åœ°æ–¹å°/å››å·.txt"},
    "henan": {"lines": [], "title": "âš”ï¸æ²³å—é¢‘é“", "file": "åœ°æ–¹å°/æ²³å—.txt"},
    "hunan": {"lines": [], "title": "ğŸŒ¶ï¸æ¹–å—é¢‘é“", "file": "åœ°æ–¹å°/æ¹–å—.txt"},
    "chongqing": {"lines": [], "title": "ğŸ²é‡åº†é¢‘é“", "file": "åœ°æ–¹å°/é‡åº†.txt"},
    "tianjin": {"lines": [], "title": "ğŸš¢å¤©æ´¥é¢‘é“", "file": "åœ°æ–¹å°/å¤©æ´¥.txt"},
    "hubei": {"lines": [], "title": "ğŸŒ‰æ¹–åŒ—é¢‘é“", "file": "åœ°æ–¹å°/æ¹–åŒ—.txt"},
    "anhui": {"lines": [], "title": "ğŸŒ¾å®‰å¾½é¢‘é“", "file": "åœ°æ–¹å°/å®‰å¾½.txt"},
    "fujian": {"lines": [], "title": "ğŸŒŠç¦å»ºé¢‘é“", "file": "åœ°æ–¹å°/ç¦å»º.txt"},
    "liaoning": {"lines": [], "title": "ğŸ­è¾½å®é¢‘é“", "file": "åœ°æ–¹å°/è¾½å®.txt"},
    "shaanxi": {"lines": [], "title": "ğŸ—¿é™•è¥¿é¢‘é“", "file": "åœ°æ–¹å°/é™•è¥¿.txt"},
    "hebei": {"lines": [], "title": "â›©ï¸æ²³åŒ—é¢‘é“", "file": "åœ°æ–¹å°/æ²³åŒ—.txt"},
    "jiangxi": {"lines": [], "title": "ğŸ¶æ±Ÿè¥¿é¢‘é“", "file": "åœ°æ–¹å°/æ±Ÿè¥¿.txt"},
    "guangxi": {"lines": [], "title": "ğŸ’ƒå¹¿è¥¿é¢‘é“", "file": "åœ°æ–¹å°/å¹¿è¥¿.txt"},
    "yunnan": {"lines": [], "title": "â˜ï¸äº‘å—é¢‘é“", "file": "åœ°æ–¹å°/äº‘å—.txt"},
    "shanxi": {"lines": [], "title": "ğŸ®å±±è¥¿é¢‘é“", "file": "åœ°æ–¹å°/å±±è¥¿.txt"},
    "heilongjiang": {"lines": [], "title": "â„ï¸é»‘Â·é¾™Â·æ±Ÿ", "file": "åœ°æ–¹å°/é»‘é¾™æ±Ÿ.txt"},
    "jilin": {"lines": [], "title": "ğŸå‰æ—é¢‘é“", "file": "åœ°æ–¹å°/å‰æ—.txt"},
    "guizhou": {"lines": [], "title": "ğŸŒˆè´µå·é¢‘é“", "file": "åœ°æ–¹å°/è´µå·.txt"},
    "gansu": {"lines": [], "title": "ğŸ«ç”˜è‚ƒé¢‘é“", "file": "åœ°æ–¹å°/ç”˜è‚ƒ.txt"},
    "neimenggu": {"lines": [], "title": "ğŸå†…Â·è’™Â·å¤", "file": "åœ°æ–¹å°/å†…è’™å¤.txt"},
    "xinjiang": {"lines": [], "title": "ğŸ‡æ–°ç–†é¢‘é“", "file": "åœ°æ–¹å°/æ–°ç–†.txt"},
    "hainan": {"lines": [], "title": "ğŸï¸æµ·å—é¢‘é“", "file": "åœ°æ–¹å°/æµ·å—.txt"},
    "ningxia": {"lines": [], "title": "ğŸ•Œå®å¤é¢‘é“", "file": "åœ°æ–¹å°/å®å¤.txt"},
    "qinghai": {"lines": [], "title": "ğŸ‘é’æµ·é¢‘é“", "file": "åœ°æ–¹å°/é’æµ·.txt"},
    "xizang": {"lines": [], "title": "ğŸè¥¿è—é¢‘é“", "file": "åœ°æ–¹å°/è¥¿è—.txt"},
    
    # æ¸¯æ¾³å°
    "hongkong": {"lines": [], "title": "ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“", "file": "åœ°æ–¹å°/é¦™æ¸¯.txt"},
    "macau": {"lines": [], "title": "ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“", "file": "åœ°æ–¹å°/æ¾³é—¨.txt"},
    "taiwan": {"lines": [], "title": "ğŸ‡¨ğŸ‡³å°æ¹¾é¢‘é“", "file": "åœ°æ–¹å°/å°æ¹¾.txt"},
    
    # å®šåˆ¶åˆ†ç±»
    "digital": {"lines": [], "title": "ğŸ“¶æ•°å­—é¢‘é“", "file": "ä¸»é¢‘é“/æ•°å­—.txt"},
    "movie": {"lines": [], "title": "ğŸ¬ç”µå½±é¢‘é“", "file": "ä¸»é¢‘é“/ç”µå½±.txt"},
    "tv_drama": {"lines": [], "title": "ğŸ“ºç”µÂ·è§†Â·å‰§", "file": "ä¸»é¢‘é“/ç”µè§†å‰§.txt"},
    "documentary": {"lines": [], "title": "ğŸ“½ï¸çºªÂ·å½•Â·ç‰‡", "file": "ä¸»é¢‘é“/çºªå½•ç‰‡.txt"},
    "cartoon": {"lines": [], "title": "ğŸ¦ŠåŠ¨Â·ç”»Â·ç‰‡", "file": "ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt"},
    "radio": {"lines": [], "title": "ğŸ“»æ”¶Â·éŸ³Â·æœº", "file": "ä¸»é¢‘é“/æ”¶éŸ³æœº.txt"},
    "variety": {"lines": [], "title": "ğŸ­ç»¼è‰ºé¢‘é“", "file": "ä¸»é¢‘é“/ç»¼è‰º.txt"},
    "huya": {"lines": [], "title": "ğŸ¯è™ç‰™ç›´æ’­", "file": "ä¸»é¢‘é“/è™ç‰™.txt"},
    "douyu": {"lines": [], "title": "ğŸ æ–—é±¼ç›´æ’­", "file": "ä¸»é¢‘é“/æ–—é±¼.txt"},
    "commentary": {"lines": [], "title": "ğŸ¤è§£è¯´é¢‘é“", "file": "ä¸»é¢‘é“/è§£è¯´.txt"},
    "music": {"lines": [], "title": "ğŸµéŸ³ä¹é¢‘é“", "file": "ä¸»é¢‘é“/éŸ³ä¹.txt"},
    "food": {"lines": [], "title": "ğŸœç¾é£Ÿé¢‘é“", "file": "ä¸»é¢‘é“/ç¾é£Ÿ.txt"},
    "travel": {"lines": [], "title": "âœˆï¸æ—…æ¸¸é¢‘é“", "file": "ä¸»é¢‘é“/æ—…æ¸¸.txt"},
    "health": {"lines": [], "title": "ğŸ¥å¥åº·é¢‘é“", "file": "ä¸»é¢‘é“/å¥åº·.txt"},
    "finance": {"lines": [], "title": "ğŸ’°è´¢ç»é¢‘é“", "file": "ä¸»é¢‘é“/è´¢ç».txt"},
    "shopping": {"lines": [], "title": "ğŸ›ï¸è´­ç‰©é¢‘é“", "file": "ä¸»é¢‘é“/è´­ç‰©.txt"},
    "game": {"lines": [], "title": "ğŸ®æ¸¸æˆé¢‘é“", "file": "ä¸»é¢‘é“/æ¸¸æˆ.txt"},
    "news": {"lines": [], "title": "ğŸ“°æ–°é—»é¢‘é“", "file": "ä¸»é¢‘é“/æ–°é—».txt"},
    "china": {"lines": [], "title": "ğŸ‡¨ğŸ‡³ä¸­å›½ç»¼åˆ", "file": "ä¸»é¢‘é“/ä¸­å›½.txt"},
    "international": {"lines": [], "title": "ğŸŒå›½é™…é¢‘é“", "file": "ä¸»é¢‘é“/å›½é™….txt"},
    "sports": {"lines": [], "title": "âš½ï¸ä½“è‚²é¢‘é“", "file": "ä¸»é¢‘é“/ä½“è‚².txt"},
    "tyss": {"lines": [], "title": "ğŸ†ï¸ä½“è‚²èµ›äº‹", "file": "ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt"},
    "mgss": {"lines": [], "title": "ğŸˆå’ªå’•èµ›äº‹", "file": "ä¸»é¢‘é“/å’ªå’•èµ›äº‹.txt"},
    "traditional_opera": {"lines": [], "title": "ğŸ­æˆæ›²é¢‘é“", "file": "ä¸»é¢‘é“/æˆæ›².txt"},
    "spring_festival_gala": {"lines": [], "title": "ğŸ§¨å†å±Šæ˜¥æ™š", "file": "ä¸»é¢‘é“/æ˜¥æ™š.txt"},
    "camera": {"lines": [], "title": "ğŸï¸æ™¯åŒºç›´æ’­", "file": "ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt"},
    "favorite": {"lines": [], "title": "â­æ”¶è—é¢‘é“", "file": "ä¸»é¢‘é“/æ”¶è—é¢‘é“.txt"},
}

# ========= åˆ†ç±»æ˜¾ç¤ºé¡ºåº =========
CATEGORY_ORDER = [
    # ä¸»é¢‘é“
    "yangshi", "weishi",
    
    # åœ°æ–¹å°
    "beijing", "shanghai", "guangdong", "jiangsu", "zhejiang",
    "shandong", "sichuan", "henan", "hunan", "chongqing",
    "tianjin", "hubei", "anhui", "fujian", "liaoning", "shaanxi",
    "hebei", "jiangxi", "guangxi", "yunnan", "shanxi", "heilongjiang",
    "jilin", "guizhou", "gansu", "neimenggu", "xinjiang", "hainan",
    "ningxia", "qinghai", "xizang",
    
    # æ¸¯æ¾³å°
    "hongkong", "macau", "taiwan",
    
    # å®šåˆ¶åˆ†ç±»
    "digital", "movie", "tv_drama", "documentary", "cartoon", "radio",
    "variety", "huya", "douyu", "commentary", "music", "food", "travel",
    "health", "finance", "shopping", "game", "news", "china", "international",
    "sports", "tyss", "mgss", "traditional_opera", "spring_festival_gala",
    "camera", "favorite",
]

# ========= å…¨å±€çŠ¶æ€ç±» =========
class GlobalState:
    def __init__(self):
        self.start_time = None
        self.processed_urls = set()  # å…¨å±€URLå»é‡é›†åˆ
        self.combined_blacklist = set()  # åˆå¹¶é»‘åå•
        self.corrections_name = {}  # é¢‘é“åç§°çº é”™å­—å…¸
        self.other_lines = []  # å…¶ä»–é¢‘é“è¡Œ
        self.other_lines_url = set()  # å…¶ä»–é¢‘é“URLï¼ˆç”¨äºå»é‡ï¼‰
        self.manual_sources = {}  # æ‰‹å·¥åŒºæº
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,  # æ€»å¤„ç†URLæ•°
            'blacklisted': 0,      # é»‘åå•è¿‡æ»¤æ•°
            'categories': {}       # å„åˆ†ç±»ç»Ÿè®¡
        }

g = GlobalState()

# ========= å·¥å…·å‡½æ•° =========
def traditional_to_simplified(text: str) -> str:
    """ç¹ä½“è½¬ç®€ä½“"""
    converter = opencc.OpenCC('t2s')
    return converter.convert(text)

def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    utc_now = datetime.now(timezone.utc)
    return utc_now + timedelta(hours=8)

def read_txt_to_array(file_name):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹åˆ°æ•°ç»„"""
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines if line.strip()]
            return lines
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯ {file_name}: {e}")
        return []

def get_random_user_agent():
    """è·å–éšæœºUser-Agent"""
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

def clean_url(url):
    """æ¸…ç†URLï¼ˆç§»é™¤$åçš„å‚æ•°ï¼‰"""
    last_dollar_index = url.rfind('$')
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

def get_url_file_extension(url):
    """è·å–URLæ–‡ä»¶æ‰©å±•å"""
    parsed_url = urlparse(url)
    path = parsed_url.path
    extension = os.path.splitext(path)[1]
    return extension

def convert_m3u_to_txt(m3u_content):
    """å°†M3Uæ ¼å¼è½¬æ¢ä¸ºTXTæ ¼å¼"""
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        if "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    return '\n'.join(txt_lines)

def process_name_string(input_str):
    """å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²ï¼ˆä¸»è¦ç”¨äºå¤„ç†CCTVé¢‘é“åï¼‰"""
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    """å¤„ç†å•ä¸ªé¢‘é“åç§°éƒ¨åˆ†"""
    if "CCTV" in part_str and "://" not in part_str:
        part_str = part_str.replace("IPV6", "")
        part_str = part_str.replace("PLUS", "+")
        part_str = part_str.replace("1080", "")
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        if not filtered_str.strip():
            filtered_str = part_str.replace("CCTV", "")
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2: 
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
        return "CCTV" + filtered_str 
    elif "å«è§†" in part_str:
        pattern = r'å«è§†ã€Œ.*ã€'
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    return part_str

# ========= é¢‘é“åç§°æ¸…ç† =========
REMOVAL_LIST = [
    "_ç”µä¿¡", "ç”µä¿¡", "é¢‘é“", "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", "ä¼é™†",
    "é™†é™†", "æŸ’é™†", "è‚†æŸ’", "é¢‘è‹±", "é¢‘ç‰¹", "é¢‘å›½", "é¢‘æ™´", "é¢‘ç²¤", "é«˜æ¸…", "è¶…æ¸…",
    "æ ‡æ¸…", "æ–¯ç‰¹", "ç²¤é™†", "å›½é™†", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹", "å’ªå’•", "é—½ç‰¹",
    "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³", "é¢‘æ•ˆ", "å›½æ ‡", "ç²¤æ ‡", "é¢‘æ¨", "é¢‘æµ", "ç²¤é«˜",
    "é¢‘é™", "å®æ—¶", "ç¾æ¨", "é¢‘ç¾", "è‹±é™†", "(åŒ—ç¾)", "ã€Œå›çœ‹ã€", "[è¶…æ¸…]", "ã€ŒIPV4ã€",
    "ã€ŒIPV6ã€", "_ITV", "(HK)", "AKtv", "HD", "[HD]", "(HD)", "ï¼ˆHDï¼‰", "{HD}", "<HD>",
    "-HD", "[BD]", "SD", "[SD]", "(SD)", "{SD}", "<SD>", "[VGA]", "4Gtv", "1080",
    "720", "480", "VGA", "4K", "(4K)", "{4K}", "<4K>", "(VGA)", "{VGA}", "<VGA>",
    "ã€Œ4gTVã€", "ã€ŒLiTVã€"
]

def clean_channel_name(channel_name, removal_list=REMOVAL_LIST):
    """æ¸…ç†é¢‘é“åç§°"""
    for item in removal_list:
        channel_name = channel_name.replace(item, "")

    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]

    return channel_name

def correct_name_data(corrections, data):
    """ä¿®æ­£é¢‘é“åç§°æ•°æ®"""
    corrected_data = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            continue
        name, url = line.split(',', 1)
        if name in corrections and name != corrections[name]:
            name = corrections[name]
        corrected_data.append(f"{name},{url}")
    return corrected_data

def sort_data(order, data):
    """æŒ‰æŒ‡å®šé¡ºåºæ’åºæ•°æ®"""
    order_dict = {name: i for i, name in enumerate(order)}
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# ========= å­—å…¸æ–‡ä»¶åŠ è½½ =========
def load_dictionaries():
    """åŠ è½½æ‰€æœ‰é¢‘é“å­—å…¸"""
    print(f"\nğŸ“š åŠ è½½é¢‘é“å­—å…¸...")
    dictionaries = {}
    
    for category_id, config in CHANNEL_CONFIG.items():
        file_path = os.path.join('assets/livesource', config['file'])
        if os.path.exists(file_path):
            dictionaries[category_id] = read_txt_to_array(file_path)
            print(f"   âœ… {config['title']}: {len(dictionaries[category_id])}æ¡")
        else:
            dictionaries[category_id] = []
            print(f"   âš ï¸  {file_path}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    print(f"âœ… å­—å…¸åŠ è½½å®Œæˆï¼Œå…± {len(dictionaries)} ä¸ªåˆ†ç±»")
    return dictionaries

def load_corrections_name():
    """åŠ è½½é¢‘é“åç§°ä¿®æ­£å­—å…¸"""
    filename = 'assets/livesource/corrections_name.txt'
    corrections = {}
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if not line or line.startswith('#'):
                    continue
                parts = line.split(',')
                if len(parts) >= 2:
                    correct_name = parts[0]
                    for name in parts[1:]:
                        if name:
                            corrections[name] = correct_name
    except FileNotFoundError:
        print(f"âš ï¸  ä¿®æ­£å­—å…¸æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
    except Exception as e:
        print(f"âŒ åŠ è½½ä¿®æ­£å­—å…¸é”™è¯¯: {e}")
    
    print(f"âœ… ä¿®æ­£å­—å…¸åŠ è½½: {len(corrections)} æ¡è§„åˆ™")
    if corrections:
        print(f"ğŸ“ ç¤ºä¾‹è§„åˆ™:")
        for i, (wrong_name, correct_name) in enumerate(list(corrections.items())[:3]):
            print(f"  {i+1}. '{wrong_name}' â†’ '{correct_name}'")
        if len(corrections) > 3:
            print(f"  ... è¿˜æœ‰ {len(corrections) - 3} æ¡")
    
    return corrections

def load_blacklist():
    """åŠ è½½é»‘åå•"""
    print(f"\nğŸš« åŠ è½½é»‘åå•...")
    
    def read_blacklist_from_txt(file_path):
        blacklist = set()
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                lines = file.readlines()
            for line in lines:
                if ',' in line:
                    url = line.split(',')[1].strip()
                    cleaned_url = clean_url(url)
                    blacklist.add(cleaned_url)
        except Exception as e:
            print(f"âŒ è¯»å–é»‘åå•é”™è¯¯ {file_path}: {e}")
        return blacklist
    
    # è¯»å–è‡ªåŠ¨å’Œæ‰‹åŠ¨ç»´æŠ¤çš„é»‘åå•
    blacklist_auto = read_blacklist_from_txt('assets/livesource/blacklist/blacklist_auto.txt') 
    blacklist_manual = read_blacklist_from_txt('assets/livesource/blacklist/blacklist_manual.txt') 
    
    # åˆå¹¶é»‘åå•
    combined_blacklist = set(blacklist_auto.union(blacklist_manual))
    
    print(f"ğŸ”§ è‡ªåŠ¨ç»´æŠ¤: {len(blacklist_auto)} æ¡")
    print(f"âœï¸ æ‰‹åŠ¨ç»´æŠ¤: {len(blacklist_manual)} æ¡")
    print(f"ğŸ”„ åˆå¹¶å»é‡: {len(combined_blacklist)} æ¡")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if combined_blacklist:
        print(f"ğŸ“‹ é»‘åå•ç¤ºä¾‹ (å‰3æ¡):")
        for i, url in enumerate(list(combined_blacklist)[:3]):
            print(f"  {i+1}. {url[:80]}..." if len(url) > 80 else f"  {i+1}. {url}")
        if len(combined_blacklist) > 3:
            print(f"  ... è¿˜æœ‰ {len(combined_blacklist) - 3} æ¡")
    
    return combined_blacklist

# ========= æ ¸å¿ƒå¤„ç†å‡½æ•° =========
def process_channel_line(line, dictionaries, is_manual=False):
    """å¤„ç†å•è¡Œé¢‘é“ä¿¡æ¯ï¼ˆv2.00é€»è¾‘ï¼Œä½¿ç”¨é…ç½®åŒ–ç»“æ„ï¼‰"""
    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é¢‘é“è¡Œ
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        # åˆ†å‰²è¡Œï¼Œè·å–åŸå§‹é¢‘é“åç§°å’ŒURL
        parts = line.split(',', 1)
        if len(parts) < 2:
            return
        
        channel_name = parts[0].strip()
        channel_address = parts[1].strip()
        
        # æ¸…ç†URL
        channel_address = clean_url(channel_address)
        
        # é»‘åå•æ£€æŸ¥
        if channel_address in g.combined_blacklist:
            print(f"ğŸš« é»‘åå•è¿‡æ»¤: {channel_name}")
            g.stats['blacklisted'] += 1
            return
        
        # å…¨å±€URLå»é‡æ£€æŸ¥
        if channel_address in g.processed_urls:
            print(f"ğŸ”„ URLå»é‡: {channel_name}")
            return
        
        g.processed_urls.add(channel_address)
        g.stats['total_processed'] += 1
        
        # æ¸…ç†é¢‘é“åç§°
        channel_name = clean_channel_name(channel_name)
        # ç¹ä½“è½¬ç®€ä½“
        channel_name = traditional_to_simplified(channel_name)
        
        # é¢‘é“åç§°çº é”™
        if channel_name in g.corrections_name:
            corrected_name = g.corrections_name[channel_name]
            if corrected_name != channel_name:
                print(f"ğŸ”§ åç§°çº é”™: {channel_name} -> {corrected_name}")
                channel_name = corrected_name
        
        # é‡æ–°ç»„åˆè¡Œ
        processed_line = channel_name + "," + channel_address
        
        # æŒ‰é…ç½®é¡ºåºåŒ¹é…åˆ†ç±»
        matched = False
        
        # ä¼˜å…ˆå¤„ç†ç‰¹æ®Šåˆ†ç±»
        # 1. å¤®è§†é¢‘é“ï¼ˆCCTVå…³é”®è¯åŒ¹é…ï¼‰
        if "CCTV" in channel_name:
            CHANNEL_CONFIG["yangshi"]["lines"].append(process_name_string(processed_line))
            matched = True
        
        # 2. å«è§†é¢‘é“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        elif not matched and channel_name in dictionaries.get("weishi", []):
            CHANNEL_CONFIG["weishi"]["lines"].append(process_name_string(processed_line))
            matched = True
        
        # 3. ä½“è‚²èµ›äº‹ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        elif not matched and any(tyss_keyword in channel_name for tyss_keyword in dictionaries.get("tyss", [])):
            CHANNEL_CONFIG["tyss"]["lines"].append(process_name_string(processed_line))
            matched = True
        
        # 4. å’ªå’•èµ›äº‹ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        elif not matched and any(mgss_keyword in channel_name for mgss_keyword in dictionaries.get("mgss", [])):
            CHANNEL_CONFIG["mgss"]["lines"].append(process_name_string(processed_line))
            matched = True
        
        # 5. å®šåˆ¶åˆ†ç±»ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        if not matched:
            for category_id in CATEGORY_ORDER:
                if category_id in ["yangshi", "weishi", "tyss", "mgss"]:
                    continue  # å·²ç»å¤„ç†è¿‡
                    
                config = CHANNEL_CONFIG[category_id]
                dict_list = dictionaries.get(category_id, [])
                
                if channel_name in dict_list:
                    config["lines"].append(process_name_string(processed_line))
                    matched = True
                    break
        
        # å¦‚æœæœªåŒ¹é…åˆ°ä»»ä½•åˆ†ç±»ï¼Œæ”¾å…¥other_lines
        if not matched:
            if channel_address not in g.other_lines_url:
                g.other_lines_url.add(channel_address)
                g.other_lines.append(processed_line)

def process_url(url, dictionaries):
    """å¤„ç†å•ä¸ªURLï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    try:
        g.other_lines.append("â—†â—†â—†ã€€" + url)
        req = urllib.request.Request(url)
        req.add_header('User-Agent', get_random_user_agent())

        with urllib.request.urlopen(req) as response:
            data = response.read()
            text = data.decode('utf-8')
            text = text.strip()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºM3Uæ ¼å¼
            is_m3u = text.startswith("#EXTM3U") or text.startswith("#EXTINF")
            if get_url_file_extension(url) in [".m3u", ".m3u8"] or is_m3u:
                text = convert_m3u_to_txt(text)

            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")

            for line in lines:
                if "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    channel_name, channel_address = line.split(',', 1)
                    if "#" not in channel_address:
                        process_channel_line(line, dictionaries)
                    else:
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline = f'{channel_name},{channel_url}'
                            process_channel_line(newline, dictionaries)

            g.other_lines.append('\n')

    except Exception as e:
        print(f"âŒ å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# ========= ç™½åå•å¤„ç† =========
def process_whitelist(dictionaries):
    """å¤„ç†ç™½åå•è‡ªåŠ¨æ–‡ä»¶ï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    print(f"\nğŸŸ¢ å¤„ç†ç™½åå•è‡ªåŠ¨æ–‡ä»¶...")
    whitelist_auto_lines = read_txt_to_array('assets/livesource/blacklist/whitelist_auto.txt')
    
    print(f"ğŸ“– è¯»å–åˆ° {len(whitelist_auto_lines)} æ¡è®°å½•")
    print(f"â­ï¸ è·³è¿‡æ ‡é¢˜è¡Œå’Œè¡¨å¤´...")
    
    valid_whitelist_count = 0
    valid_whitelist_samples = []
    
    for i, whitelist_line in enumerate(whitelist_auto_lines):
        if i < 2:  # è·³è¿‡å‰ä¸¤è¡Œï¼ˆæ ‡é¢˜å’Œæ—¥æœŸè¡Œï¼‰
            continue
        
        # è·³è¿‡è¡¨å¤´è¡Œ
        if whitelist_line.startswith("RespoTime,whitelist,#genre#"):
            continue
            
        # å¤„ç†çœŸæ­£çš„ç™½åå•è¡Œ
        if "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
            whitelist_parts = whitelist_line.split(",")
            if len(whitelist_parts) >= 3:
                valid_whitelist_count += 1
                
                # ä¿å­˜ç¤ºä¾‹
                if len(valid_whitelist_samples) < 3:
                    valid_whitelist_samples.append(whitelist_line)
                
                try:
                    response_time = float(whitelist_parts[0].replace("ms", ""))
                except ValueError:
                    print(f"âŒ response_timeè½¬æ¢å¤±è´¥: {whitelist_line}")
                    response_time = 60000
                
                # åªå¤„ç†å“åº”æ—¶é—´å°äº2ç§’çš„é«˜è´¨é‡æº
                if response_time < 2000:
                    process_channel_line(",".join(whitelist_parts[1:]), dictionaries)
    
    print(f"æœ‰æ•ˆç™½åå•è®°å½•: {valid_whitelist_count} æ¡")
    if valid_whitelist_samples:
        print(f"ğŸ“‹ ç™½åå•ç¤ºä¾‹ (å‰3æ¡):")
        for i, line in enumerate(valid_whitelist_samples[:3]):
            truncated = line[:80] + "..." if len(line) > 80 else line
            print(f"  {i+1}. {truncated}")
        if valid_whitelist_count > 3:
            print(f"  ... è¿˜æœ‰ {valid_whitelist_count - 3} æ¡")
    print()

# ========= AKTVç‰¹æ®Šå¤„ç† =========
def process_aktv(dictionaries):
    """å¤„ç†AKTVç›´æ’­æºï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    print(f"ğŸ“º è·å–AKTVç›´æ’­æº...")
    
    # AKTVæºåœ°å€
    aktv_url = "https://raw.githubusercontent.com/xiaoran67/update/refs/heads/main/assets/livesource/blacklist/whitelist_manual.txt"
    
    # å°è¯•ä»ç½‘ç»œè·å–
    def get_http_response(url, timeout=8, retries=2):
        headers = {'User-Agent': get_random_user_agent()}
        for attempt in range(retries):
            try:
                req = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(req, timeout=timeout) as response:
                    data = response.read()
                    return data.decode('utf-8')
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(1.0 * (2 ** attempt))
                else:
                    print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")
        return None
    
    aktv_text = get_http_response(aktv_url)
    aktv_lines = []
    
    if aktv_text:
        print(f"âœ… AKTVæˆåŠŸè·å–å†…å®¹")
        aktv_text = convert_m3u_to_txt(aktv_text)
        aktv_lines = aktv_text.strip().split('\n')
    else:
        print(f"âš ï¸ AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
        aktv_lines = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/AKTV.txt')
    
    print(f"å¤„ç†AKTVæ•°æ®ï¼Œå…± {len(aktv_lines)} è¡Œ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š AKTVé¢‘é“ç»Ÿè®¡:")
    if aktv_lines:
        print(f"ğŸ“‹ AKTVé¢‘é“ç¤ºä¾‹ (å‰3æ¡):")
        for i, line in enumerate(aktv_lines[:3]):
            truncated = line[:60] + "..." if len(line) > 60 else line
            print(f"  {i+1}. {truncated}")
        if len(aktv_lines) > 3:
            print(f"  ... è¿˜æœ‰ {len(aktv_lines) - 3} æ¡")
    
    # å¤„ç†AKTVæ•°æ®
    for line in aktv_lines:
        process_channel_line(line, dictionaries)

# ========= æ‰‹å·¥åŒºå¤„ç† =========
def process_manual_sources():
    """å¤„ç†æ‰‹å·¥åŒºé«˜è´¨é‡æºï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    print(f"\nğŸ”§ å¤„ç†æ‰‹å·¥åŒºé«˜è´¨é‡æº...")
    
    # æ‰‹å·¥åŒºæ–‡ä»¶åˆ—è¡¨
    manual_files = {
        'zhejiang': 'æµ™æ±Ÿé¢‘é“.txt',
        'guangdong': 'å¹¿ä¸œé¢‘é“.txt',
        'hubei': 'æ¹–åŒ—é¢‘é“.txt',
        'shanghai': 'ä¸Šæµ·é¢‘é“.txt',
        'jiangsu': 'æ±Ÿè‹é¢‘é“.txt'
    }
    
    total_manual = 0
    for region, filename in manual_files.items():
        filepath = f'assets/livesource/æ‰‹å·¥åŒº/{filename}'
        lines = read_txt_to_array(filepath)
        if lines:
            print(f"âœ… {filename}: {len(lines)} æ¡")
            # ç›´æ¥æ·»åŠ åˆ°å¯¹åº”åˆ†ç±»çš„è¡Œä¸­
            if region in CHANNEL_CONFIG:
                CHANNEL_CONFIG[region]["lines"].extend(lines)
                total_manual += len(lines)
        else:
            print(f"âš ï¸  {filename}: æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
    
    print(f"æ‰‹å·¥åŒºæ€»è®¡: {total_manual} æ¡")

# ========= ä½“è‚²èµ›äº‹å¤„ç† =========
def normalize_date_to_md(text):
    """å°†æ—¥æœŸæ ¼å¼è§„èŒƒåŒ–ä¸ºMM-DDæ ¼å¼ï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    text = text.strip()
    
    def format_md(m):
        month = int(m.group(1))
        day = int(m.group(2))
        after = m.group(3) or ''
        if not after.startswith(' '):
            after = ' ' + after
        return f"{month:02d}-{day:02d}{after}"
    
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)

    return text

def filter_lines(lines, exclude_keywords):
    """è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¡Œ"""
    return [line for line in lines if not any(keyword in line for keyword in exclude_keywords)]

def custom_tyss_sort(lines):
    """è‡ªå®šä¹‰ä½“è‚²èµ›äº‹æ’åºå‡½æ•°ï¼ˆæ•°å­—å‰ç¼€çš„å€’åºï¼Œå…¶ä»–æ­£åºï¼‰"""
    digit_prefix = []
    others = []
    
    for line in lines:
        name_part = line.split(',')[0].strip()
        if name_part and name_part[0].isdigit():
            digit_prefix.append(line)
        else:
            others.append(line)
    
    digit_prefix_sorted = sorted(digit_prefix, reverse=True)
    others_sorted = sorted(others)

    return digit_prefix_sorted + others_sorted

def generate_playlist_html(data_list, output_file='output/tiyu.html'):
    """ç”Ÿæˆä½“è‚²èµ›äº‹HTMLé¡µé¢ï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Setup Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''
    
    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''
    
    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… ä½“è‚²èµ›äº‹ç½‘é¡µå·²ç”Ÿæˆï¼š{output_file}")

def process_tyss_data():
    """å¤„ç†ä½“è‚²èµ›äº‹æ•°æ®ï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    print(f"\nğŸ† å¤„ç†ä½“è‚²èµ›äº‹æ•°æ®...")
    
    # ä»é…ç½®ä¸­è·å–ä½“è‚²èµ›äº‹è¡Œ
    tyss_lines = CHANNEL_CONFIG["tyss"]["lines"]
    
    if not tyss_lines:
        print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä½“è‚²èµ›äº‹æ•°æ®")
        return None
    
    # æ—¥æœŸæ ¼å¼åŒ–
    normalized_tyss_lines = [normalize_date_to_md(s) for s in tyss_lines]
    
    # è¿‡æ»¤å…³é”®è¯
    keywords_to_exclude_tiyu_txt = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†", "å…¬ä¼—å·", "éº»è±†", "ã€Œå›çœ‹ã€"]
    keywords_to_exclude_tiyu = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†", "å…¬ä¼—å·", "å’ªè§†é€š", "éº»è±†", "ã€Œå›çœ‹ã€"]
    
    # åº”ç”¨è¿‡æ»¤
    normalized_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu_txt)
    
    # å»é‡å¹¶æ’åº
    normalized_tyss_lines = custom_tyss_sort(set(normalized_tyss_lines))
    
    # è¿›ä¸€æ­¥è¿‡æ»¤
    filtered_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu)
    
    print(f"âœ… ä½“è‚²èµ›äº‹å¤„ç†å®Œæˆï¼šåŸå§‹ {len(tyss_lines)} æ¡ï¼Œè¿‡æ»¤å {len(filtered_tyss_lines)} æ¡")
    
    # ç”ŸæˆHTMLæ–‡ä»¶
    generate_playlist_html(filtered_tyss_lines, 'output/tiyu.html')
    
    # ç”ŸæˆTXTæ–‡ä»¶
    with open('output/tiyu.txt', 'w', encoding='utf-8') as f:
        for line in filtered_tyss_lines:
            f.write(line + '\n')
    print(f"âœ… ä½“è‚²èµ›äº‹æ–‡æœ¬å·²ç”Ÿæˆ: output/tiyu.txt")
    
    return filtered_tyss_lines

# ========= ä»Šæ—¥æ¨èå’Œç‰ˆæœ¬ä¿¡æ¯ =========
def get_random_url(file_path):
    """ä»æ–‡ä»¶ä¸­éšæœºè·å–URLï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    urls = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                if ',' in line:
                    url = line.strip().split(',')[-1]
                    urls.append(url)
    except Exception as e:
        print(f"âŒ è¯»å–éšæœºURLæ–‡ä»¶é”™è¯¯ {file_path}: {e}")
    return random.choice(urls) if urls else ""

# ========= ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ =========
def generate_output_files(filtered_tyss_lines=None):
    """æ ¹æ®é…ç½®ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ï¼ˆåŸºäºv2.00é€»è¾‘ï¼Œä½¿ç”¨é…ç½®åŒ–ç»“æ„ï¼‰"""
    print(f"\nğŸ“ ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
    
    # è¯»å–æ‰‹å·¥åŒºæ–‡ä»¶
    zhuanxiang_yangshi = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å¤®è§†.txt')
    zhuanxiang_weishi = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å«è§†.txt')
    about_info = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/about.txt')
    aktv_lines = read_txt_to_array('assets/livesource/æ‰‹å·¥åŒº/AKTV.txt')
    
    # ç”Ÿæˆä»Šæ—¥æ¨èå’Œç‰ˆæœ¬ä¿¡æ¯
    beijing_time = get_beijing_time()
    formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")
    
    # ä»Šæ—¥æ¨è
    MTV1 = "ğŸ’¯æ¨è," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt') or "")
    MTV2 = "ğŸ¤«ä½è°ƒ," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt') or "")
    MTV3 = "ğŸŸ¢ä½¿ç”¨," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt') or "")
    MTV4 = "âš ï¸ç¦æ­¢," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt') or "")
    MTV5 = "ğŸš«è´©å–," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt') or "")
    
    # ç‰ˆæœ¬ä¿¡æ¯
    version = formatted_time + "," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt') or "")
    about = "ğŸ‘¨æ½‡ç„¶," + (get_random_url('assets/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt') or "")
    
    # æ„å»ºå®Œæ•´ç‰ˆæ’­æ”¾åˆ—è¡¨
    playlist_full = []
    
    # æŒ‰é…ç½®é¡ºåºæ·»åŠ åˆ†ç±»
    for category_id in CATEGORY_ORDER:
        if category_id in CHANNEL_CONFIG:
            config = CHANNEL_CONFIG[category_id]
            lines = config["lines"]
            if lines:
                # å¯¹æ¯ä¸ªåˆ†ç±»çš„è¡Œè¿›è¡Œå»é‡å’Œæ’åº
                dict_path = os.path.join('assets/livesource', config['file'])
                if os.path.exists(dict_path):
                    order_list = read_txt_to_array(dict_path)
                    sorted_lines = sort_data(order_list, correct_name_data(g.corrections_name, lines))
                    playlist_full.append(f"{config['title']},#genre#")
                    playlist_full.extend(sorted_lines)
                    playlist_full.append('')
                else:
                    # å¦‚æœæ²¡æœ‰å­—å…¸æ–‡ä»¶ï¼Œä½¿ç”¨ç®€å•æ’åº
                    playlist_full.append(f"{config['title']},#genre#")
                    playlist_full.extend(sorted(set(correct_name_data(g.corrections_name, lines))))
                    playlist_full.append('')
    
    # æ·»åŠ ä¸“äº«å¤®è§†å’Œå«è§†
    if zhuanxiang_yangshi:
        playlist_full.append("ğŸ‘‘ä¸“äº«å¤®è§†,#genre#")
        playlist_full.extend(zhuanxiang_yangshi)
        playlist_full.append('')
    
    if zhuanxiang_weishi:
        playlist_full.append("â˜•ï¸ä¸“äº«å«è§†,#genre#")
        playlist_full.extend(zhuanxiang_weishi)
        playlist_full.append('')
    
    # æ·»åŠ ä½“è‚²èµ›äº‹ï¼ˆå¦‚æœå·²å¤„ç†ï¼‰
    if filtered_tyss_lines:
        playlist_full.append("ğŸ†ï¸ä½“è‚²èµ›äº‹,#genre#")
        playlist_full.extend(filtered_tyss_lines)
        playlist_full.append('')
    
#    # æ·»åŠ AKTVæº
#    if aktv_lines:
#        playlist_full.append("ğŸš€ FreeTV,#genre#")
#        playlist_full.extend(aktv_lines)
#        playlist_full.append('')
    
    # æ·»åŠ å…¶ä»–åˆ†ç±»
    if g.other_lines:
        playlist_full.append("ğŸ“¦å…¶ä»–é¢‘é“,#genre#")
        playlist_full.extend(sorted(set(g.other_lines)))
        playlist_full.append('')
    
    # æ·»åŠ æ›´æ–°æ—¶é—´
    playlist_full.append("ğŸ•’æ›´æ–°æ—¶é—´,#genre#")
    playlist_full.append(version)
    playlist_full.append(about)
    playlist_full.append(MTV1)
    playlist_full.append(MTV2)
    playlist_full.append(MTV3)
    playlist_full.append(MTV4)
    playlist_full.append(MTV5)
    playlist_full.extend(about_info)
    playlist_full.append('')
    
    # æ„å»ºç²¾ç®€ç‰ˆæ’­æ”¾åˆ—è¡¨
    playlist_lite = []
    
    # 1. å¤®è§†é¢‘é“
    yangshi_config = CHANNEL_CONFIG["yangshi"]
    yangshi_lines_list = yangshi_config["lines"]
    if yangshi_lines_list:
        yangshi_dict_path = os.path.join('assets/livesource', yangshi_config['file'])
        if os.path.exists(yangshi_dict_path):
            yangshi_order = read_txt_to_array(yangshi_dict_path)
            yangshi_sorted = sort_data(yangshi_order, correct_name_data(g.corrections_name, yangshi_lines_list))
            playlist_lite.append(f"{yangshi_config['title']},#genre#")
            playlist_lite.extend(yangshi_sorted)
            playlist_lite.append('')
    
    # 2. å«è§†é¢‘é“
    weishi_config = CHANNEL_CONFIG["weishi"]
    weishi_lines_list = weishi_config["lines"]
    if weishi_lines_list:
        weishi_dict_path = os.path.join('assets/livesource', weishi_config['file'])
        if os.path.exists(weishi_dict_path):
            weishi_order = read_txt_to_array(weishi_dict_path)
            weishi_sorted = sort_data(weishi_order, correct_name_data(g.corrections_name, weishi_lines_list))
            playlist_lite.append(f"{weishi_config['title']},#genre#")
            playlist_lite.extend(weishi_sorted)
            playlist_lite.append('')
    
    # 3. åœ°æ–¹å°ï¼ˆåˆå¹¶ä¸ºä¸€ä¸ªåˆ†ç±»ï¼‰
    print(f"ğŸ  åˆå¹¶åœ°æ–¹å°é¢‘é“...")
    local_categories = [
        "beijing", "shanghai", "guangdong", "jiangsu", "zhejiang",
        "shandong", "sichuan", "henan", "hunan", "chongqing",
        "tianjin", "hubei", "anhui", "fujian", "liaoning", "shaanxi",
        "hebei", "jiangxi", "guangxi", "yunnan", "shanxi", "heilongjiang",
        "jilin", "guizhou", "gansu", "neimenggu", "xinjiang", "hainan",
        "ningxia", "qinghai", "xizang"
    ]
    
    all_local_lines = []
    for local_id in local_categories:
        if local_id in CHANNEL_CONFIG:
            local_config = CHANNEL_CONFIG[local_id]
            local_lines = local_config["lines"]
            if local_lines:
                local_dict_path = os.path.join('assets/livesource', local_config['file'])
                if os.path.exists(local_dict_path):
                    local_order = read_txt_to_array(local_dict_path)
                    local_sorted = sort_data(local_order, correct_name_data(g.corrections_name, local_lines))
                    all_local_lines.extend(local_sorted)
    
    if all_local_lines:
        playlist_lite.append("ğŸ åœ°Â·æ–¹Â·å°,#genre#")
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        deduplicated_lines = []
        for line in all_local_lines:
            if line not in seen:
                seen.add(line)
                deduplicated_lines.append(line)
        playlist_lite.extend(deduplicated_lines)
        playlist_lite.append('')
        print(f"åœ°æ–¹å°é¢‘é“: {len(deduplicated_lines)} ä¸ª")
    
    # 4. æ·»åŠ æ›´æ–°æ—¶é—´
    playlist_lite.append("ğŸ•’æ›´æ–°æ—¶é—´,#genre#")
    playlist_lite.append(version)
    playlist_lite.append(about)
    playlist_lite.append(MTV1)
    playlist_lite.append(MTV2)
    playlist_lite.append(MTV3)
    playlist_lite.append(MTV4)
    playlist_lite.append(MTV5)
    playlist_lite.extend(about_info)
    playlist_lite.append('')
    
    # ========= æ„å»ºå®šåˆ¶ç‰ˆæ’­æ”¾åˆ—è¡¨ï¼ˆä¸v2.00ä¿æŒä¸€è‡´ï¼‰ =========
    playlist_custom = []
    
    # 1. å¤®è§†é¢‘é“
    if 'yangshi_sorted' in locals():
        playlist_custom.append("ğŸŒå¤®è§†é¢‘é“,#genre#")
        playlist_custom.extend(yangshi_sorted)
        playlist_custom.append('')
    
    # 2. å«è§†é¢‘é“
    if 'weishi_sorted' in locals():
        playlist_custom.append("ğŸ“¡å«è§†é¢‘é“,#genre#")
        playlist_custom.extend(weishi_sorted)
        playlist_custom.append('')
    
    # 3. åœ°æ–¹å°
    if all_local_lines:
        playlist_custom.append("ğŸ åœ°Â·æ–¹Â·å°,#genre#")
        playlist_custom.extend(deduplicated_lines)
        playlist_custom.append('')
    
    # 4. æ¸¯æ¾³å°
    for category_id in ["hongkong", "macau", "taiwan"]:
        if category_id in CHANNEL_CONFIG:
            config = CHANNEL_CONFIG[category_id]
            lines = config["lines"]
            if lines:
                dict_path = os.path.join('assets/livesource', config['file'])
                if os.path.exists(dict_path):
                    order_list = read_txt_to_array(dict_path)
                    sorted_lines = sort_data(order_list, correct_name_data(g.corrections_name, lines))
                    playlist_custom.append(f"{config['title']},#genre#")
                    playlist_custom.extend(sorted_lines)
                    playlist_custom.append('')
    
    # 5. å®šåˆ¶åˆ†ç±»
    other_categories = [
        "digital", "movie", "tv_drama", "documentary", "cartoon", "radio",
        "variety", "huya", "douyu", "commentary", "music", "food", "travel",
        "health", "finance", "shopping", "game", "news", "china", "international",
        "sports", "traditional_opera", "spring_festival_gala", "favorite"
    ]
    
    for category_id in other_categories:
        if category_id in CHANNEL_CONFIG:
            config = CHANNEL_CONFIG[category_id]
            lines = config["lines"]
            if lines:
                dict_path = os.path.join('assets/livesource', config['file'])
                if os.path.exists(dict_path):
                    order_list = read_txt_to_array(dict_path)
                    sorted_lines = sort_data(order_list, correct_name_data(g.corrections_name, lines))
                    playlist_custom.append(f"{config['title']},#genre#")
                    playlist_custom.extend(sorted_lines)
                    playlist_custom.append('')
                else:
                    # å¦‚æœæ²¡æœ‰å­—å…¸æ–‡ä»¶ï¼Œä½¿ç”¨ç®€å•æ’åº
                    playlist_custom.append(f"{config['title']},#genre#")
                    playlist_custom.extend(sorted(set(correct_name_data(g.corrections_name, lines))))
                    playlist_custom.append('')
    
    # 6. ä½“è‚²èµ›äº‹å’Œå’ªå’•èµ›äº‹
    if filtered_tyss_lines:
        playlist_custom.append("ğŸ†ï¸ä½“è‚²èµ›äº‹,#genre#")
        playlist_custom.extend(filtered_tyss_lines)
        playlist_custom.append('')
    
    mgss_config = CHANNEL_CONFIG["mgss"]
    if mgss_config["lines"]:
        mgss_dict_path = os.path.join('assets/livesource', mgss_config['file'])
        if os.path.exists(mgss_dict_path):
            mgss_order = read_txt_to_array(mgss_dict_path)
            mgss_sorted = sort_data(mgss_order, correct_name_data(g.corrections_name, mgss_config["lines"]))
            playlist_custom.append("ğŸˆå’ªå’•èµ›äº‹,#genre#")
            playlist_custom.extend(mgss_sorted)
            playlist_custom.append('')
    
    # 7. ä¸“äº«å¤®è§†å’Œå«è§†
    if zhuanxiang_yangshi:
        playlist_custom.append("ğŸ‘‘ä¸“äº«å¤®è§†,#genre#")
        playlist_custom.extend(zhuanxiang_yangshi)
        playlist_custom.append('')
    
    if zhuanxiang_weishi:
        playlist_custom.append("â˜•ï¸ä¸“äº«å«è§†,#genre#")
        playlist_custom.extend(zhuanxiang_weishi)
        playlist_custom.append('')
    
    # 8. æ™¯åŒºç›´æ’­
    camera_config = CHANNEL_CONFIG["camera"]
    if camera_config["lines"]:
        playlist_custom.append("ğŸï¸æ™¯åŒºç›´æ’­,#genre#")
        playlist_custom.extend(sorted(set(correct_name_data(g.corrections_name, camera_config["lines"]))))
        playlist_custom.append('')
    
    # 9. å…¶ä»–åˆ†ç±»
    if g.other_lines:
        playlist_custom.append("ğŸ“¦å…¶ä»–é¢‘é“,#genre#")
        playlist_custom.extend(sorted(set(g.other_lines)))
        playlist_custom.append('')
    
    # 10. æ›´æ–°æ—¶é—´
    playlist_custom.append("ğŸ•’æ›´æ–°æ—¶é—´,#genre#")
    playlist_custom.append(version)
    playlist_custom.append(about)
    playlist_custom.append(MTV1)
    playlist_custom.append(MTV2)
    playlist_custom.append(MTV3)
    playlist_custom.append(MTV4)
    playlist_custom.append(MTV5)
    playlist_custom.extend(about_info)
    playlist_custom.append('')
    
    # å®šä¹‰è¾“å‡ºæ–‡ä»¶å
    output_full = "output/full.txt"
    output_lite = "output/lite.txt"
    output_custom = "output/custom.txt"
    output_others = "output/others.txt"
    
    # å†™å…¥æ–‡ä»¶
    try:
        with open(output_full, 'w', encoding='utf-8') as f:
            for line in playlist_full:
                f.write(line + '\n')
        print(f"âœ… å®Œæ•´ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_full}")
        
        with open(output_lite, 'w', encoding='utf-8') as f:
            for line in playlist_lite:
                f.write(line + '\n')
        print(f"âœ… ç²¾ç®€ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_lite}")
        
        with open(output_custom, 'w', encoding='utf-8') as f:
            for line in playlist_custom:
                f.write(line + '\n')
        print(f"âœ… å®šåˆ¶ç‰ˆæ’­æ”¾åˆ—è¡¨å·²ä¿å­˜: {output_custom}")
        
        with open(output_others, 'w', encoding='utf-8') as f:
            for line in g.other_lines:
                f.write(line + '\n')
        print(f"âœ… æœªåˆ†ç±»é¢‘é“åˆ—è¡¨å·²ä¿å­˜: {output_others}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return 0, 0, 0

    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return len(playlist_full), len(playlist_lite), len(playlist_custom)

# ========= ç”ŸæˆM3Uæ ¼å¼æ–‡ä»¶ =========
def make_m3u(txt_file, m3u_file):
    """å°†TXTæ–‡ä»¶è½¬æ¢ä¸ºM3Uæ ¼å¼ï¼ˆåŸºäºv2.00é€»è¾‘ï¼‰"""
    try:
        channels_logos = read_txt_to_array('assets/livesource/logo.txt')
        
        def get_logo_by_channel_name(channel_name):
            for line in channels_logos:
                if not line.strip():
                    continue
                if ',' in line:
                    name, url = line.split(',', 1)
                    if name == channel_name:
                        return url
            return None
        
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'
        
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()
        
        lines = input_text.strip().split("\n")
        group_name = ""
        
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = get_logo_by_channel_name(channel_name)
                
                if logo_url is None:
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1 tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\" group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
        
        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)
        
        print(f"â–¶ï¸ M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ ç”ŸæˆM3Uæ–‡ä»¶é”™è¯¯: {e}")

# ========= ä¸»å‡½æ•° =========
def main():
    """ä¸»å‡½æ•°"""
    print()
    print(f"=" * 31)
    print(f"ğŸ IPTVç›´æ’­æºèšåˆå¤„ç†å·¥å…· v3.02")
    print(f"ğŸ“º Live Source Collector")
    print(f"ğŸ‰ åŸºäºv2.00ï¼Œé‡æ„ç‰ˆ")
    print(f"=" * 31)
    
    # æ‰§è¡Œå¼€å§‹æ—¶é—´
    g.start_time = get_beijing_time()
    print(f"\nâ° å¼€å§‹æ—¶é—´: {g.start_time.strftime('%Y%m%d %H:%M:%S')}")
    
    # 1. åŠ è½½å­—å…¸
    dictionaries = load_dictionaries()
    
    # 2. åŠ è½½é»‘åå•
    g.combined_blacklist = load_blacklist()
    
    # 3. åŠ è½½ä¿®æ­£å­—å…¸
    g.corrections_name = load_corrections_name()
    
    # 4. å¤„ç†URLåˆ—è¡¨
    urls = read_txt_to_array('assets/livesource/urls-daily.txt')
    print(f"\nğŸ“¡ å¼€å§‹å¤„ç† {len(urls)} ä¸ªæ•°æ®è®¢é˜…æº")
    
    for url in urls:
        if url.startswith("http"):
            # å¤„ç†æ—¥æœŸå ä½ç¬¦
            if "{MMdd}" in url:
                current_date_str = get_beijing_time().strftime("%m%d")
                url = url.replace("{MMdd}", current_date_str)
            if "{MMdd-1}" in url:
                yesterday_date_str = (get_beijing_time() - timedelta(days=1)).strftime("%m%d")
                url = url.replace("{MMdd-1}", yesterday_date_str)
            print(f"ğŸ“¡ å¤„ç†URL: {url}")

            process_url(url, dictionaries)
    
    print(f"âœ… URLå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(urls)} ä¸ªæ•°æ®æº")
    
    # 5. å¤„ç†ç™½åå•
    process_whitelist(dictionaries)
    
    # 6. å¤„ç†AKTVæº
    process_aktv(dictionaries)
    
    # 7. å¤„ç†æ‰‹å·¥åŒºæº
    process_manual_sources()
    
    # 8. å¤„ç†ä½“è‚²èµ›äº‹
    filtered_tyss_lines = process_tyss_data()
    
    # 9. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¹¶è·å–ç»Ÿè®¡å€¼
    full_count, lite_count, custom_count = generate_output_files(filtered_tyss_lines)
    
    # 10. ç”ŸæˆM3Uæ–‡ä»¶
    make_m3u("output/full.txt", "output/full.m3u")
    make_m3u("output/lite.txt", "output/lite.m3u")
    make_m3u("output/custom.txt", "output/custom.m3u")
    
    # 11. ç»Ÿè®¡ä¿¡æ¯
    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    timeend = get_beijing_time()
    elapsed_time = timeend - g.start_time
    total_seconds = elapsed_time.total_seconds()
    minutes = int(total_seconds // 60)
    seconds = int(total_seconds % 60)

    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   å¼€å§‹æ—¶é—´: {g.start_time.strftime('%Y%m%d %H:%M:%S')}")
    print(f"   ç»“æŸæ—¶é—´: {timeend.strftime('%Y%m%d %H:%M:%S')}")
    print(f"   æ‰§è¡Œæ—¶é—´: {minutes}åˆ†{seconds}ç§’")

    # è®¡ç®—å¤„ç†é€Ÿåº¦
    if total_seconds > 0:
        channels_per_second = g.stats['total_processed'] / total_seconds
        print(f"   å¤„ç†é€Ÿåº¦: {channels_per_second:.1f} é¢‘é“/ç§’")

    # URLå»é‡ç»Ÿè®¡
    processed_urls_count = len(g.processed_urls)
    blacklist_urls_count = len(g.combined_blacklist)
    total_processed_urls = processed_urls_count + blacklist_urls_count

    print(f"\nğŸ”„ å»é‡ç»Ÿè®¡:")
    print(f"   å”¯ä¸€çš„URLæ•°: {processed_urls_count}")
    print(f"   é»‘åå•URLæ•°: {blacklist_urls_count}")
    print(f"   æ€»å¤„ç†URLæ•°: {total_processed_urls}")

    if total_processed_urls > 0:
        duplication_rate = (1 - processed_urls_count / total_processed_urls) * 100
        print(f"   ğŸ”„ å»é‡ç‡: {duplication_rate:.1f}%")

    # é¢‘é“æ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“¦ æ•°æ®ç»Ÿè®¡:")
    print(f"   é»‘åå•æ¡æ•°: {len(g.combined_blacklist)}")
    print(f"   å…¶ä»–æœªåˆ†ç±»: {len(g.other_lines)}")
    print(f"   ä½“è‚²èµ›äº‹æ•°: {len(filtered_tyss_lines) if filtered_tyss_lines else 0}")
    print(f"   å®Œæ•´ç‰ˆæ¡æ•°: {full_count}")
    print(f"   ç²¾ç®€ç‰ˆæ¡æ•°: {lite_count}")
    print(f"   å®šåˆ¶ç‰ˆæ¡æ•°: {custom_count}")

    # é¢‘é“åˆ†ç±»ç»Ÿè®¡
    print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    total_channels = 0
    for category_id in CATEGORY_ORDER:
        if category_id in CHANNEL_CONFIG:
            config = CHANNEL_CONFIG[category_id]
            count = len(config["lines"])
            if count > 0:
                print(f"   {config['title']}: {count}ä¸ªé¢‘é“")
                total_channels += count

    print(f"\nğŸ“Š æ€»è®¡: {total_channels} ä¸ªé¢‘é“")

    print(f"\nğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ!âœ…ğŸš€")

# ========= ç¨‹åºå…¥å£ =========
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  1. ä¿®æ”¹ CHANNEL_CONFIG å¯ä»¥å¢åˆ æ”¹åˆ†ç±»")
    print(f"  2. ä¿®æ”¹ CATEGORY_ORDER å¯ä»¥è°ƒæ•´æ˜¾ç¤ºé¡ºåº")
    print(f"  3. é‡æ–°è¿è¡Œè„šæœ¬å³å¯åº”ç”¨æ–°é…ç½®")
    print(f"=" * 31)

# ====== ç›´æ’­æºèšåˆå¤„ç†å·¥å…· v3.02 ======